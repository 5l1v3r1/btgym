
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    <title>btgym.algorithms package &#8212; BTgym 0.0.6 documentation</title>
    <link rel="stylesheet" href="_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    './',
        VERSION:     '0.0.6',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true,
        SOURCELINK_SUFFIX: '.txt'
      };
    </script>
    <script type="text/javascript" src="_static/jquery.js"></script>
    <script type="text/javascript" src="_static/underscore.js"></script>
    <script type="text/javascript" src="_static/doctools.js"></script>
    <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="prev" title="btgym.rendering package" href="btgym.rendering.html" /> 
  </head>
  <body>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="btgym.rendering.html" title="btgym.rendering package"
             accesskey="P">previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">BTgym 0.0.6 documentation</a> &#187;</li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <div class="section" id="btgym-algorithms-package">
<h1>btgym.algorithms package<a class="headerlink" href="#btgym-algorithms-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-btgym.algorithms.launcher">
<span id="btgym-algorithms-launcher-module"></span><h2>btgym.algorithms.launcher module<a class="headerlink" href="#module-btgym.algorithms.launcher" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.launcher.Launcher">
<em class="property">class </em><code class="descclassname">btgym.algorithms.launcher.</code><code class="descname">Launcher</code><span class="sig-paren">(</span><em>root_random_seed=None</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/launcher.html#Launcher"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.launcher.Launcher" title="Permalink to this definition">¶</a></dt>
<dd><p>Configures and starts distributed TF training session with workers
running separate instances of BTgym/Atari environment.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>root_random_seed</strong> – int, random seed.</li>
<li><strong>**kwargs</strong> – passed to worker, trainer, environment runner.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="attribute">
<dt id="btgym.algorithms.launcher.Launcher.policy_class">
<code class="descname">policy_class</code><a class="headerlink" href="#btgym.algorithms.launcher.Launcher.policy_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal"><span class="pre">BaseAacAuxPolicy</span></code></p>
</dd></dl>

<dl class="attribute">
<dt id="btgym.algorithms.launcher.Launcher.trainer_class">
<code class="descname">trainer_class</code><a class="headerlink" href="#btgym.algorithms.launcher.Launcher.trainer_class" title="Permalink to this definition">¶</a></dt>
<dd><p>alias of <code class="xref py py-class docutils literal"><span class="pre">A3C</span></code></p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.launcher.Launcher.make_cluster_spec">
<code class="descname">make_cluster_spec</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/launcher.html#Launcher.make_cluster_spec"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.launcher.Launcher.make_cluster_spec" title="Permalink to this definition">¶</a></dt>
<dd><p>Composes cluster specification dictionary.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.launcher.Launcher.clear_port">
<code class="descname">clear_port</code><span class="sig-paren">(</span><em>port</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/launcher.html#Launcher.clear_port"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.launcher.Launcher.clear_port" title="Permalink to this definition">¶</a></dt>
<dd><p>Kills process on specified port, if any.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.launcher.Launcher.run">
<code class="descname">run</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/launcher.html#Launcher.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.launcher.Launcher.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Launches processes:
tf distributed workers;
tf parameter_server.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.worker">
<span id="btgym-algorithms-worker-module"></span><h2>btgym.algorithms.worker module<a class="headerlink" href="#module-btgym.algorithms.worker" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.worker.FastSaver">
<em class="property">class </em><code class="descclassname">btgym.algorithms.worker.</code><code class="descname">FastSaver</code><span class="sig-paren">(</span><em>var_list=None</em>, <em>reshape=False</em>, <em>sharded=False</em>, <em>max_to_keep=5</em>, <em>keep_checkpoint_every_n_hours=10000.0</em>, <em>name=None</em>, <em>restore_sequentially=False</em>, <em>saver_def=None</em>, <em>builder=None</em>, <em>defer_build=False</em>, <em>allow_empty=False</em>, <em>write_version=2</em>, <em>pad_step_number=False</em>, <em>save_relative_paths=False</em>, <em>filename=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/worker.html#FastSaver"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.worker.FastSaver" title="Permalink to this definition">¶</a></dt>
<dd><p>Disables write_meta_graph argument,
which freezes entire process and is mostly useless.</p>
<p>Creates a <cite>Saver</cite>.</p>
<p>The constructor adds ops to save and restore variables.</p>
<p><cite>var_list</cite> specifies the variables that will be saved and restored. It can
be passed as a <cite>dict</cite> or a list:</p>
<ul class="simple">
<li>A <cite>dict</cite> of names to variables: The keys are the names that will be
used to save or restore the variables in the checkpoint files.</li>
<li>A list of variables: The variables will be keyed with their op name in
the checkpoint files.</li>
</ul>
<p>For example:</p>
<p><a href="#id1"><span class="problematic" id="id2">``</span></a><a href="#id3"><span class="problematic" id="id4">`</span></a>python
v1 = tf.Variable(…, name=’v1’)
v2 = tf.Variable(…, name=’v2’)</p>
<p># Pass the variables as a dict:
saver = tf.train.Saver({‘v1’: v1, ‘v2’: v2})</p>
<p># Or pass them as a list.
saver = tf.train.Saver([v1, v2])
# Passing a list is equivalent to passing a dict with the variable op names
# as keys:
saver = tf.train.Saver({v.op.name: v for v in [v1, v2]})
<a href="#id5"><span class="problematic" id="id6">``</span></a><a href="#id7"><span class="problematic" id="id8">`</span></a></p>
<p>The optional <cite>reshape</cite> argument, if <cite>True</cite>, allows restoring a variable from
a save file where the variable had a different shape, but the same number
of elements and type.  This is useful if you have reshaped a variable and
want to reload it from an older checkpoint.</p>
<p>The optional <cite>sharded</cite> argument, if <cite>True</cite>, instructs the saver to shard
checkpoints per device.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>var_list</strong> – A list of <cite>Variable</cite>/<cite>SaveableObject</cite>, or a dictionary mapping
names to <cite>SaveableObject`s. If `None</cite>, defaults to the list of all
saveable objects.</li>
<li><strong>reshape</strong> – If <cite>True</cite>, allows restoring parameters from a checkpoint
where the variables have a different shape.</li>
<li><strong>sharded</strong> – If <cite>True</cite>, shard the checkpoints, one per device.</li>
<li><strong>max_to_keep</strong> – Maximum number of recent checkpoints to keep.
Defaults to 5.</li>
<li><strong>keep_checkpoint_every_n_hours</strong> – How often to keep checkpoints.
Defaults to 10,000 hours.</li>
<li><strong>name</strong> – String.  Optional name to use as a prefix when adding operations.</li>
<li><strong>restore_sequentially</strong> – A <cite>Bool</cite>, which if true, causes restore of different
variables to happen sequentially within each device.  This can lower
memory usage when restoring very large models.</li>
<li><strong>saver_def</strong> – Optional <cite>SaverDef</cite> proto to use instead of running the
builder. This is only useful for specialty code that wants to recreate
a <cite>Saver</cite> object for a previously built <cite>Graph</cite> that had a <cite>Saver</cite>.
The <cite>saver_def</cite> proto should be the one returned by the
<cite>as_saver_def()</cite> call of the <cite>Saver</cite> that was created for that <cite>Graph</cite>.</li>
<li><strong>builder</strong> – Optional <cite>SaverBuilder</cite> to use if a <cite>saver_def</cite> was not provided.
Defaults to <cite>BaseSaverBuilder()</cite>.</li>
<li><strong>defer_build</strong> – If <cite>True</cite>, defer adding the save and restore ops to the
<cite>build()</cite> call. In that case <cite>build()</cite> should be called before
finalizing the graph or using the saver.</li>
<li><strong>allow_empty</strong> – If <cite>False</cite> (default) raise an error if there are no
variables in the graph. Otherwise, construct the saver anyway and make
it a no-op.</li>
<li><strong>write_version</strong> – controls what format to use when saving checkpoints.  It
also affects certain filepath matching logic.  The V2 format is the
recommended choice: it is much more optimized than V1 in terms of
memory required and latency incurred during restore.  Regardless of
this flag, the Saver is able to restore from both V2 and V1 checkpoints.</li>
<li><strong>pad_step_number</strong> – if True, pads the global step number in the checkpoint
filepaths to some fixed width (8 by default).  This is turned off by
default.</li>
<li><strong>save_relative_paths</strong> – If <cite>True</cite>, will write relative paths to the
checkpoint state file. This is needed if the user wants to copy the
checkpoint directory and reload from the copied directory.</li>
<li><strong>filename</strong> – If known at graph construction time, filename used for variable
loading/saving.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Raises:</th><td class="field-body"><ul class="first last simple">
<li><code class="xref py py-exc docutils literal"><span class="pre">TypeError</span></code> – If <cite>var_list</cite> is invalid.</li>
<li><code class="xref py py-exc docutils literal"><span class="pre">ValueError</span></code> – If any of the keys or values in <cite>var_list</cite> are not unique.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.worker.Worker">
<em class="property">class </em><code class="descclassname">btgym.algorithms.worker.</code><code class="descname">Worker</code><span class="sig-paren">(</span><em>env_class</em>, <em>env_config</em>, <em>policy_class</em>, <em>policy_config</em>, <em>trainer_class</em>, <em>cluster_spec</em>, <em>job_name</em>, <em>task</em>, <em>log_dir</em>, <em>log</em>, <em>log_level</em>, <em>max_steps=1000000000</em>, <em>test_mode=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/worker.html#Worker"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.worker.Worker" title="Permalink to this definition">¶</a></dt>
<dd><p>Distributed tf worker class.</p>
<p>Sets up environment, trainer and starts training process in supervised session.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_class</strong> – environment class to use.</li>
<li><strong>env_config</strong> – configuration dictionary.</li>
<li><strong>policy_class</strong> – model policy estimator class to use.</li>
<li><strong>policy_config</strong> – configuration dictionary.</li>
<li><strong>trainer_class</strong> – algorithm class to use.</li>
<li><strong>cluster_spec</strong> – tf.cluster specification.</li>
<li><strong>job_name</strong> – worker or parameter server.</li>
<li><strong>task</strong> – integer number, 0 is chief worker.</li>
<li><strong>log_dir</strong> – for tb summaries and checkpoints.</li>
<li><strong>log</strong> – </li>
<li><strong>log_level</strong> – </li>
<li><strong>max_steps</strong> – number of train steps</li>
<li><strong>test_mode</strong> – if True - use Atari mode, BTGym otherwise.</li>
<li><strong>**kwargs</strong> – args passed to trainer.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.worker.Worker.run">
<code class="descname">run</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/worker.html#Worker.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.worker.Worker.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Worker runtime body.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.runner">
<span id="btgym-algorithms-runner-module"></span><h2>btgym.algorithms.runner module<a class="headerlink" href="#module-btgym.algorithms.runner" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.runner.RunnerThread">
<em class="property">class </em><code class="descclassname">btgym.algorithms.runner.</code><code class="descname">RunnerThread</code><span class="sig-paren">(</span><em>env</em>, <em>policy</em>, <em>task</em>, <em>rollout_length</em>, <em>episode_summary_freq</em>, <em>env_render_freq</em>, <em>test</em>, <em>ep_summary</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/runner.html#RunnerThread"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.runner.RunnerThread" title="Permalink to this definition">¶</a></dt>
<dd><p>Async. framework code comes from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<p>Despite the fact BTgym is not real-time environment [yet], thread-runner approach is still here.</p>
<p>From original <cite>universe-starter-agent</cite>:
…One of the key distinctions between a normal environment and a universe environment
is that a universe environment is _real <a href="#id13"><span class="problematic" id="id14">time_</span></a>.  This means that there should be a thread
that would constantly interact with the environment and tell it what to do.  This thread is here.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – environment instance</li>
<li><strong>policy</strong> – policy instance</li>
<li><strong>task</strong> – int</li>
<li><strong>rollout_length</strong> – int</li>
<li><strong>episode_summary_freq</strong> – int</li>
<li><strong>env_render_freq</strong> – int</li>
<li><strong>test</strong> – Atari or BTGyn</li>
<li><strong>ep_summary</strong> – tf.summary</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.runner.RunnerThread.run">
<code class="descname">run</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/runner.html#RunnerThread.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.runner.RunnerThread.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Just keep running.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.runner.env_runner">
<code class="descclassname">btgym.algorithms.runner.</code><code class="descname">env_runner</code><span class="sig-paren">(</span><em>sess</em>, <em>env</em>, <em>policy</em>, <em>task</em>, <em>rollout_length</em>, <em>summary_writer</em>, <em>episode_summary_freq</em>, <em>env_render_freq</em>, <em>test</em>, <em>ep_summary</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/runner.html#env_runner"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.runner.env_runner" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic of the thread runner.
In brief, it constantly keeps on running
the policy, and as long as the rollout exceeds a certain length, the thread
runner appends the rollout to the queue.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – environment instance</li>
<li><strong>policy</strong> – policy instance</li>
<li><strong>task</strong> – int</li>
<li><strong>rollout_length</strong> – int</li>
<li><strong>episode_summary_freq</strong> – int</li>
<li><strong>env_render_freq</strong> – int</li>
<li><strong>test</strong> – Atari or BTGyn</li>
<li><strong>ep_summary</strong> – tf.summary</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Yelds:</dt>
<dd>rollout instance</dd>
</dl>
</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.a3c">
<span id="btgym-algorithms-a3c-module"></span><h2>btgym.algorithms.a3c module<a class="headerlink" href="#module-btgym.algorithms.a3c" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.a3c.A3C">
<em class="property">class </em><code class="descclassname">btgym.algorithms.a3c.</code><code class="descname">A3C</code><span class="sig-paren">(</span><em>env</em>, <em>task</em>, <em>policy_class</em>, <em>policy_config</em>, <em>log</em>, <em>random_seed=None</em>, <em>model_gamma=0.99</em>, <em>model_gae_lambda=1.0</em>, <em>model_beta=0.01</em>, <em>opt_max_train_steps=10000000</em>, <em>opt_decay_steps=None</em>, <em>opt_end_learn_rate=None</em>, <em>opt_learn_rate=0.0001</em>, <em>opt_decay=0.99</em>, <em>opt_momentum=0.0</em>, <em>opt_epsilon=1e-10</em>, <em>rollout_length=20</em>, <em>episode_summary_freq=2</em>, <em>env_render_freq=10</em>, <em>model_summary_freq=100</em>, <em>test_mode=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/a3c.html#A3C"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.a3c.A3C" title="Permalink to this definition">¶</a></dt>
<dd><p>Asynchronous Advantage Actor Critic.</p>
<p>Original code is taken from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<p>Paper: <a class="reference external" href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – envirionment instance.</li>
<li><strong>task</strong> – int</li>
<li><strong>policy_class</strong> – policy estimator class</li>
<li><strong>policy_config</strong> – config dictionary</li>
<li><strong>log</strong> – parent log</li>
<li><strong>random_seed</strong> – int or None</li>
<li><strong>model_gamma</strong> – gamma discount factor</li>
<li><strong>model_gae_lambda</strong> – GAE lambda</li>
<li><strong>model_beta</strong> – entropy regularization beta</li>
<li><strong>opt_max_train_steps</strong> – train steps to run</li>
<li><strong>opt_decay_steps</strong> – learn ratio decay steps</li>
<li><strong>opt_end_learn_rate</strong> – final lerarn rate</li>
<li><strong>opt_learn_rate</strong> – start learn rate</li>
<li><strong>opt_decay</strong> – optimizer decay, if apll.</li>
<li><strong>opt_momentum</strong> – optimizer momentum, if apll.</li>
<li><strong>opt_epsilon</strong> – optimizer epsilon</li>
<li><strong>rollout_length</strong> – on-policy rollout length</li>
<li><strong>episode_summary_freq</strong> – int, write episode summary for every i’th episode</li>
<li><strong>env_render_freq</strong> – int, write environment rendering summary for every i’th train step</li>
<li><strong>model_summary_freq</strong> – int, write model summary for every i’th train step</li>
<li><strong>test_mode</strong> – True: Atari, False: BTGym</li>
<li><strong>**kwargs</strong> – NOT USED</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.a3c.A3C.pull_batch_from_queue">
<code class="descname">pull_batch_from_queue</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/a3c.html#A3C.pull_batch_from_queue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.a3c.A3C.pull_batch_from_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Self explanatory:  take a rollout from the queue of the thread runner.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.a3c.A3C.process">
<code class="descname">process</code><span class="sig-paren">(</span><em>sess</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/a3c.html#A3C.process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.a3c.A3C.process" title="Permalink to this definition">¶</a></dt>
<dd><p>Grabs a on_policy_rollout that’s been produced by the thread runner,
samples off_policy rollout[s] from replay memory and updates the parameters.
The update is then sent to the parameter server.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.envs">
<span id="btgym-algorithms-envs-module"></span><h2>btgym.algorithms.envs module<a class="headerlink" href="#module-btgym.algorithms.envs" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="btgym.algorithms.envs.create_env">
<code class="descclassname">btgym.algorithms.envs.</code><code class="descname">create_env</code><span class="sig-paren">(</span><em>env_id</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/envs.html#create_env"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.envs.create_env" title="Permalink to this definition">¶</a></dt>
<dd><p>State preprocessor wrapper for Atari environments.</p>
</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.losses">
<span id="btgym-algorithms-losses-module"></span><h2>btgym.algorithms.losses module<a class="headerlink" href="#module-btgym.algorithms.losses" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="btgym.algorithms.losses.aac_loss_def">
<code class="descclassname">btgym.algorithms.losses.</code><code class="descname">aac_loss_def</code><span class="sig-paren">(</span><em>act_target</em>, <em>adv_target</em>, <em>r_target</em>, <em>pi_logits</em>, <em>pi_vf</em>, <em>entropy_beta</em>, <em>name='aac'</em>, <em>verbose=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/losses.html#aac_loss_def"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.losses.aac_loss_def" title="Permalink to this definition">¶</a></dt>
<dd><p>Advantage Actor Critic loss definition.
Paper: <a class="reference external" href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor holding policy actions targets;</strong> (<em>act_target</em>) – </li>
<li><strong>tensor holding policy estimated advantages targets;</strong> (<em>adv_target</em>) – </li>
<li><strong>tensor holding policy empirical returns targets;</strong> (<em>r_target</em>) – </li>
<li><strong>policy logits output tensor;</strong> (<em>pi__logits</em>) – </li>
<li><strong>policy value function output tensor;</strong> (<em>pi_vf</em>) – </li>
<li><strong>entropy regularization constant;</strong> (<em>entropy_beta</em>) – </li>
<li><strong>scope;</strong> (<em>name</em>) – </li>
<li><strong>summary level.</strong> (<em>verbose</em>) – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">tensor holding estimated AAC loss;
list of related tensorboard summaries.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.losses.ppo_loss_def">
<code class="descclassname">btgym.algorithms.losses.</code><code class="descname">ppo_loss_def</code><span class="sig-paren">(</span><em>act_target</em>, <em>adv_target</em>, <em>r_target</em>, <em>pi_logits</em>, <em>pi_vf</em>, <em>pi_old_logits</em>, <em>entropy_beta</em>, <em>epsilon</em>, <em>name='ppo'</em>, <em>verbose=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/losses.html#ppo_loss_def"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.losses.ppo_loss_def" title="Permalink to this definition">¶</a></dt>
<dd><p>PPO clipped surrogate loss definition, as (7) in <a class="reference external" href="https://arxiv.org/pdf/1707.06347.pdf">https://arxiv.org/pdf/1707.06347.pdf</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor holding policy actions targets;</strong> (<em>act_target</em>) – </li>
<li><strong>tensor holding policy estimated advantages targets;</strong> (<em>adv_target</em>) – </li>
<li><strong>tensor holding policy empirical returns targets;</strong> (<em>r_target</em>) – </li>
<li><strong>policy logits output tensor;</strong> (<em>pi__logits</em>) – </li>
<li><strong>policy value function output tensor;</strong> (<em>pi_vf</em>) – </li>
<li><strong>old_policy logits output tensor;</strong> (<em>pi_old_logits</em>) – </li>
<li><strong>entropy regularization constant</strong> (<em>entropy_beta</em>) – </li>
<li><strong>L^Clip epsilon tensor;</strong> (<em>epsilon</em>) – </li>
<li><strong>scope;</strong> (<em>name</em>) – </li>
<li><strong>summary level.</strong> (<em>verbose</em>) – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">tensor holding estimated PPO L^Clip loss;
list of related tensorboard summaries.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.losses.value_fn_loss_def">
<code class="descclassname">btgym.algorithms.losses.</code><code class="descname">value_fn_loss_def</code><span class="sig-paren">(</span><em>r_target</em>, <em>pi_vf</em>, <em>name='value_replay'</em>, <em>verbose=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/losses.html#value_fn_loss_def"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.losses.value_fn_loss_def" title="Permalink to this definition">¶</a></dt>
<dd><p>Value function loss.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor holding policy empirical returns targets;</strong> (<em>r_target</em>) – </li>
<li><strong>policy value function output tensor;</strong> (<em>pi_vf</em>) – </li>
<li><strong>scope;</strong> (<em>name</em>) – </li>
<li><strong>summary level.</strong> (<em>verbose</em>) – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">tensor holding estimated value fn. loss;
list of related tensorboard summaries.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.losses.pc_loss_def">
<code class="descclassname">btgym.algorithms.losses.</code><code class="descname">pc_loss_def</code><span class="sig-paren">(</span><em>actions</em>, <em>targets</em>, <em>pi_pc_q</em>, <em>name='pixel_control'</em>, <em>verbose=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/losses.html#pc_loss_def"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.losses.pc_loss_def" title="Permalink to this definition">¶</a></dt>
<dd><p>Pixel control auxiliary task loss definition.</p>
<p>Paper: <a class="reference external" href="https://arxiv.org/abs/1611.05397">https://arxiv.org/abs/1611.05397</a></p>
<p>Borrows heavily from Kosuke Miyoshi code, under Apache License 2.0:</p>
<p><a class="reference external" href="https://miyosuda.github.io/">https://miyosuda.github.io/</a></p>
<p><a class="reference external" href="https://github.com/miyosuda/unreal">https://github.com/miyosuda/unreal</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor holding policy actions;</strong> (<em>actions</em>) – </li>
<li><strong>tensor holding estimated pixel-change targets;</strong> (<em>targets</em>) – </li>
<li><strong>policy Q-value features output tensor;</strong> (<em>pi_pc_q</em>) – </li>
<li><strong>scope;</strong> (<em>name</em>) – </li>
<li><strong>summary level.</strong> (<em>verbose</em>) – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">tensor holding estimated pc loss;
list of related tensorboard summaries.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.losses.rp_loss_def">
<code class="descclassname">btgym.algorithms.losses.</code><code class="descname">rp_loss_def</code><span class="sig-paren">(</span><em>rp_targets</em>, <em>pi_rp_logits</em>, <em>name='reward_prediction'</em>, <em>verbose=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/losses.html#rp_loss_def"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.losses.rp_loss_def" title="Permalink to this definition">¶</a></dt>
<dd><p>Reward prediction auxillary task loss definition.</p>
<p>Paper: <a class="reference external" href="https://arxiv.org/abs/1611.05397">https://arxiv.org/abs/1611.05397</a></p>
<p>Borrows heavily from Kosuke Miyoshi code, under Apache License 2.0:</p>
<p><a class="reference external" href="https://miyosuda.github.io/">https://miyosuda.github.io/</a></p>
<p><a class="reference external" href="https://github.com/miyosuda/unreal">https://github.com/miyosuda/unreal</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor holding reward prediction target;</strong> (<em>targets</em>) – </li>
<li><strong>policy reward predictions tensor;</strong> (<em>pi_rp_logits</em>) – </li>
<li><strong>scope;</strong> (<em>name</em>) – </li>
<li><strong>summary level.</strong> (<em>verbose</em>) – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">tensor holding estimated rp loss;
list of related tensorboard summaries.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.losses.aac_ppo_loss_def">
<code class="descclassname">btgym.algorithms.losses.</code><code class="descname">aac_ppo_loss_def</code><span class="sig-paren">(</span><em>act_target</em>, <em>adv_target</em>, <em>r_target</em>, <em>pi_logits</em>, <em>pi_vf</em>, <em>pi_old_logits</em>, <em>entropy_beta</em>, <em>epsilon</em>, <em>name='ppo'</em>, <em>verbose=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/losses.html#aac_ppo_loss_def"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.losses.aac_ppo_loss_def" title="Permalink to this definition">¶</a></dt>
<dd><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">IGNORE!! Do not use this loss.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>tensor holding policy actions targets;</strong> (<em>act_target</em>) – </li>
<li><strong>tensor holding policy estimated advantages targets;</strong> (<em>adv_target</em>) – </li>
<li><strong>tensor holding policy empirical returns targets;</strong> (<em>r_target</em>) – </li>
<li><strong>policy logits output tensor;</strong> (<em>pi__logits</em>) – </li>
<li><strong>policy value function output tensor;</strong> (<em>pi_vf</em>) – </li>
<li><strong>old_policy logits output tensor;</strong> (<em>pi_old_logits</em>) – </li>
<li><strong>entropy regularization constant</strong> (<em>entropy_beta</em>) – </li>
<li><strong>L^Clip epsilon tensor;</strong> (<em>epsilon</em>) – </li>
<li><strong>scope;</strong> (<em>name</em>) – </li>
<li><strong>summary level.</strong> (<em>verbose</em>) – </li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">tensor holding estimated PPO L^Clip loss;
list of related tensorboard summaries.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.memory">
<span id="btgym-algorithms-memory-module"></span><h2>btgym.algorithms.memory module<a class="headerlink" href="#module-btgym.algorithms.memory" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.memory.Memory">
<em class="property">class </em><code class="descclassname">btgym.algorithms.memory.</code><code class="descname">Memory</code><span class="sig-paren">(</span><em>history_size</em>, <em>max_sample_size</em>, <em>log</em>, <em>reward_threshold=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Bases: <a class="reference external" href="https://docs.python.org/2/library/functions.html#object" title="(in Python v2.7)"><code class="xref py py-class docutils literal"><span class="pre">object</span></code></a></p>
<p>Replay memory with rebalanced replay.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">must be filled up before calling sampling methods.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>history_size</strong> – number of experiences stored;</li>
<li><strong>max_sample_size</strong> – maximum allowed sample size (e.g. off-policy rollout length);</li>
<li><strong>reward_threshold</strong> – if <a href="#id11"><span class="problematic" id="id12">|experience.reward|</span></a> &gt; reward_threshold: experience is saved as ‘prioritized’;</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.memory.Memory.add_frame">
<code class="descname">add_frame</code><span class="sig-paren">(</span><em>frame</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.add_frame"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.add_frame" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends single experience frame to memory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>frame</strong> – dictionary of values.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory.add_rollout">
<code class="descname">add_rollout</code><span class="sig-paren">(</span><em>rollout</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.add_rollout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.add_rollout" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds frames from given rollout to memory with respect to episode continuation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>rollout</strong> – <cite>Rollout</cite> instance.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory.is_full">
<code class="descname">is_full</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.is_full"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.is_full" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory.sample_uniform">
<code class="descname">sample_uniform</code><span class="sig-paren">(</span><em>sequence_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.sample_uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.sample_uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Uniformly samples sequence of successive frames of size <cite>sequence_size</cite> or less (~off-policy rollout).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sequence_size</strong> – maximum sample size.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">list of ExperienceFrame’s of length &lt;= sequence_size.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory.sample_priority">
<code class="descname">sample_priority</code><span class="sig-paren">(</span><em>size</em>, <em>exact_size=False</em>, <em>skewness=2</em>, <em>sample_attempts=100</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.sample_priority"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.sample_priority" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements rebalanced replay;
samples sequence of successive frames from distribution skewed by means of reward of last sample frame.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>size</strong> – sample size, must be &lt;= self.max_sample_size;</li>
<li><strong>exact_size</strong> – whether accept sample with size less than ‘size’
or re-sample to get sample of exact size (used for reward prediction task);</li>
<li><strong>skewness</strong> – int&gt;=1, sampling probability denominator, such as probability of sampling sequence with
last frame having non-zero reward is: p[non_zero]=1/skewness;</li>
<li><strong>sample_attempts</strong> – if exact_size=True, sets number of re-sampling attempts
to get sample of continuous experiences (no <cite>Terminal</cite> frames inside except last one);
if number is reached - sample returned ‘as is’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">list of ExperienceFrame’s.</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="btgym-algorithms-model-module">
<h2>btgym.algorithms.model module<a class="headerlink" href="#btgym-algorithms-model-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="btgym-algorithms-aac-policy-module">
<h2>btgym.algorithms.aac_policy module<a class="headerlink" href="#btgym-algorithms-aac-policy-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="btgym-algorithms-model-unreal-module">
<h2>btgym.algorithms.model_unreal module<a class="headerlink" href="#btgym-algorithms-model-unreal-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-btgym.algorithms.nnet_util">
<span id="btgym-algorithms-nnet-util-module"></span><h2>btgym.algorithms.nnet_util module<a class="headerlink" href="#module-btgym.algorithms.nnet_util" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="btgym.algorithms.nnet_util.rnn_placeholders">
<code class="descclassname">btgym.algorithms.nnet_util.</code><code class="descname">rnn_placeholders</code><span class="sig-paren">(</span><em>state</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/nnet_util.html#rnn_placeholders"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.nnet_util.rnn_placeholders" title="Permalink to this definition">¶</a></dt>
<dd><p>Given nested [multilayer] RNN state tensor, infers and returns state placeholders.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>state</strong> – tf.nn.lstm zero-state tuple.</td>
</tr>
</tbody>
</table>
<p>Returns:    tf.contrib.rnn.LSTMStateTuple of placeholders</p>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.nnet_util.linear">
<code class="descclassname">btgym.algorithms.nnet_util.</code><code class="descname">linear</code><span class="sig-paren">(</span><em>x</em>, <em>size</em>, <em>name</em>, <em>initializer=None</em>, <em>bias_init=0</em>, <em>reuse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/nnet_util.html#linear"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.nnet_util.linear" title="Permalink to this definition">¶</a></dt>
<dd><p>Linear network layer.</p>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.nnet_util.conv2d">
<code class="descclassname">btgym.algorithms.nnet_util.</code><code class="descname">conv2d</code><span class="sig-paren">(</span><em>x</em>, <em>num_filters</em>, <em>name</em>, <em>filter_size=(3</em>, <em>3)</em>, <em>stride=(1</em>, <em>1)</em>, <em>pad='SAME'</em>, <em>dtype=tf.float32</em>, <em>collections=None</em>, <em>reuse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/nnet_util.html#conv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.nnet_util.conv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>2D convolution layer.</p>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.nnet_util.deconv2d">
<code class="descclassname">btgym.algorithms.nnet_util.</code><code class="descname">deconv2d</code><span class="sig-paren">(</span><em>x</em>, <em>output_channels</em>, <em>name</em>, <em>filter_size=(4</em>, <em>4)</em>, <em>stride=(2</em>, <em>2)</em>, <em>dtype=tf.float32</em>, <em>collections=None</em>, <em>reuse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/nnet_util.html#deconv2d"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.nnet_util.deconv2d" title="Permalink to this definition">¶</a></dt>
<dd><p>Deconvolution layer, paper:
<a class="reference external" href="http://www.matthewzeiler.com/wp-content/uploads/2017/07/cvpr2010.pdf">http://www.matthewzeiler.com/wp-content/uploads/2017/07/cvpr2010.pdf</a></p>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.nnet_util.conv_2d_network">
<code class="descclassname">btgym.algorithms.nnet_util.</code><code class="descname">conv_2d_network</code><span class="sig-paren">(</span><em>x</em>, <em>ob_space</em>, <em>ac_space</em>, <em>num_layers=4</em>, <em>num_filters=32</em>, <em>filter_size=(3</em>, <em>3)</em>, <em>stride=(2</em>, <em>2)</em>, <em>pad='SAME'</em>, <em>dtype=tf.float32</em>, <em>collections=None</em>, <em>reuse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/nnet_util.html#conv_2d_network"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.nnet_util.conv_2d_network" title="Permalink to this definition">¶</a></dt>
<dd><p>Stage1 network: from preprocessed 2D input to estimated features.
Encapsulates convolutions, [possibly] skip-connections etc. Can be shared.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">tensor holding state features;</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.nnet_util.lstm_network">
<code class="descclassname">btgym.algorithms.nnet_util.</code><code class="descname">lstm_network</code><span class="sig-paren">(</span><em>x</em>, <em>a_r</em>, <em>lstm_class=&lt;class 'tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell'&gt;</em>, <em>lstm_layers=(256</em>, <em>)</em>, <em>reuse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/nnet_util.html#lstm_network"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.nnet_util.lstm_network" title="Permalink to this definition">¶</a></dt>
<dd><p>Stage2 network: from features to flattened LSTM output.
Defines [multi-layered] dynamic [possibly shared] LSTM network.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">batch-wise flattened output tensor;
lstm initial state tensor;
lstm state output tensor;
lstm flattened feed placeholders as tuple.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.nnet_util.dense_aac_network">
<code class="descclassname">btgym.algorithms.nnet_util.</code><code class="descname">dense_aac_network</code><span class="sig-paren">(</span><em>x</em>, <em>ac_space</em>, <em>reuse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/nnet_util.html#dense_aac_network"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.nnet_util.dense_aac_network" title="Permalink to this definition">¶</a></dt>
<dd><p>Stage3 network: from LSTM flattened output to advantage actor-critic.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">logits tensor
value function tensor
action sampling function.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.nnet_util.dense_rp_network">
<code class="descclassname">btgym.algorithms.nnet_util.</code><code class="descname">dense_rp_network</code><span class="sig-paren">(</span><em>x</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/nnet_util.html#dense_rp_network"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.nnet_util.dense_rp_network" title="Permalink to this definition">¶</a></dt>
<dd><p>Stage3 network: From shared convolutions to reward-prediction task output tensor.</p>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.nnet_util.pixel_change_2d_estimator">
<code class="descclassname">btgym.algorithms.nnet_util.</code><code class="descname">pixel_change_2d_estimator</code><span class="sig-paren">(</span><em>ob_space</em>, <em>stride=2</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/nnet_util.html#pixel_change_2d_estimator"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.nnet_util.pixel_change_2d_estimator" title="Permalink to this definition">¶</a></dt>
<dd><p>Defines tf op for estimating <cite>pixel change</cite> as subsampled absolute difference of two states.</p>
</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.nnet_util.duelling_pc_network">
<code class="descclassname">btgym.algorithms.nnet_util.</code><code class="descname">duelling_pc_network</code><span class="sig-paren">(</span><em>x</em>, <em>ac_space</em>, <em>reuse=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/nnet_util.html#duelling_pc_network"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.nnet_util.duelling_pc_network" title="Permalink to this definition">¶</a></dt>
<dd><p>Stage3 network for <a href="#id9"><span class="problematic" id="id10">`</span></a>pixel control’ task: from LSTM output to Q-aux. features tensor.</p>
</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.ppo">
<span id="btgym-algorithms-ppo-module"></span><h2>btgym.algorithms.ppo module<a class="headerlink" href="#module-btgym.algorithms.ppo" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.ppo.PPO">
<em class="property">class </em><code class="descclassname">btgym.algorithms.ppo.</code><code class="descname">PPO</code><span class="sig-paren">(</span><em>env</em>, <em>task</em>, <em>policy_class</em>, <em>policy_config</em>, <em>log</em>, <em>random_seed=None</em>, <em>model_gamma=0.99</em>, <em>model_gae_lambda=1.0</em>, <em>model_beta=0.01</em>, <em>clip_epsilon=0.1</em>, <em>opt_max_train_steps=10000000</em>, <em>opt_decay_steps=None</em>, <em>opt_end_learn_rate=None</em>, <em>opt_learn_rate=0.0001</em>, <em>opt_decay=0.99</em>, <em>opt_momentum=0.0</em>, <em>opt_epsilon=1e-10</em>, <em>rollout_length=20</em>, <em>num_epochs=1</em>, <em>pi_old_update_period=10</em>, <em>episode_summary_freq=2</em>, <em>env_render_freq=10</em>, <em>model_summary_freq=100</em>, <em>test_mode=False</em>, <em>replay_memory_size=2000</em>, <em>replay_rollout_length=None</em>, <em>use_off_policy_aac=False</em>, <em>use_reward_prediction=False</em>, <em>use_pixel_control=False</em>, <em>use_value_replay=False</em>, <em>use_rebalanced_replay=False</em>, <em>rebalance_skewness=2</em>, <em>rp_lambda=1</em>, <em>pc_lambda=0.1</em>, <em>vr_lambda=1</em>, <em>off_aac_lambda=1</em>, <em>gamma_pc=0.9</em>, <em>rp_reward_threshold=0.1</em>, <em>rp_sequence_size=3</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/ppo.html#PPO"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.ppo.PPO" title="Permalink to this definition">¶</a></dt>
<dd><p>Asynchronous implementation of Proximal Policy Optimization algorithm,
augmented with auxiliary Unreal-style tasks.</p>
<p>paper:
<a class="reference external" href="https://arxiv.org/pdf/1707.06347.pdf">https://arxiv.org/pdf/1707.06347.pdf</a></p>
<p>Based on PPO-SGD code from OpenAI <cite>Baselines</cite> repository under MIT licence:
<a class="reference external" href="https://github.com/openai/baselines">https://github.com/openai/baselines</a></p>
<p>Async. framework code comes from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – envirionment instance.</li>
<li><strong>task</strong> – int</li>
<li><strong>policy_class</strong> – policy estimator class</li>
<li><strong>policy_config</strong> – config dictionary</li>
<li><strong>log</strong> – parent log</li>
<li><strong>random_seed</strong> – </li>
<li><strong>model_gamma</strong> – gamma discount factor</li>
<li><strong>model_gae_lambda</strong> – GAE lambda</li>
<li><strong>model_beta</strong> – entropy regularization beta</li>
<li><strong>clip_epsilon</strong> – PPO surrogate L^clip epsilon</li>
<li><strong>opt_max_train_steps</strong> – train steps to run</li>
<li><strong>opt_decay_steps</strong> – learn ratio decay steps</li>
<li><strong>opt_end_learn_rate</strong> – final lerarn rate</li>
<li><strong>opt_learn_rate</strong> – start learn rate</li>
<li><strong>opt_decay</strong> – optimizer decay, if apll.</li>
<li><strong>opt_momentum</strong> – optimizer momentum, if apll.</li>
<li><strong>opt_epsilon</strong> – optimizer epsilon</li>
<li><strong>rollout_length</strong> – on-policy rollout length</li>
<li><strong>num_epochs</strong> – number epochs to run on every train step</li>
<li><strong>pi_old_update_period</strong> – int, PPO pi to pi old update period in number of train steps</li>
<li><strong>episode_summary_freq</strong> – int, write episode summary for every i’th episode</li>
<li><strong>env_render_freq</strong> – int, write environment rendering summary for every i’th train step</li>
<li><strong>model_summary_freq</strong> – int, write model summary for every i’th train step</li>
<li><strong>test_mode</strong> – True: Atari, False: BTGym</li>
<li><strong>replay_memory_size</strong> – in number of experiences</li>
<li><strong>replay_rollout_length</strong> – off-policy rollout length</li>
<li><strong>use_off_policy_aac</strong> – use full AAC off policy training</li>
<li><strong>use_reward_prediction</strong> – use aux. off-policy reward prediction task</li>
<li><strong>use_pixel_control</strong> – use aux. off-policy pixel control task</li>
<li><strong>use_value_replay</strong> – use aux. off-policy value replay task (not used, if use_off_policy_aac=True)</li>
<li><strong>use_rebalanced_replay</strong> – NOT USED</li>
<li><strong>rebalance_skewness</strong> – NOT USED</li>
<li><strong>rp_lambda</strong> – reward prediction loss weight</li>
<li><strong>pc_lambda</strong> – pixel control loss weight</li>
<li><strong>vr_lambda</strong> – value replay loss weight</li>
<li><strong>off_aac_lambda</strong> – off-policy AAC loss weight</li>
<li><strong>gamma_pc</strong> – NOT USED</li>
<li><strong>rp_reward_threshold</strong> – reward prediction task classification threshold, above which reward is ‘non-zero’</li>
<li><strong>rp_sequence_size</strong> – reward prediction sample size, in number of experiences</li>
<li><strong>**kwargs</strong> – NOT USED</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.ppo.PPO.pull_batch_from_queue">
<code class="descname">pull_batch_from_queue</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/ppo.html#PPO.pull_batch_from_queue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.ppo.PPO.pull_batch_from_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Self explanatory:  take a rollout from the queue of the thread runner.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.ppo.PPO.process_rp">
<code class="descname">process_rp</code><span class="sig-paren">(</span><em>rp_experience_frames</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/ppo.html#PPO.process_rp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.ppo.PPO.process_rp" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimates reward prediction target.
Returns feed dictionary for <cite>reward prediction</cite> loss estimation subgraph.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.ppo.PPO.process_vr">
<code class="descname">process_vr</code><span class="sig-paren">(</span><em>batch</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/ppo.html#PPO.process_vr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.ppo.PPO.process_vr" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns feed dictionary for <cite>value replay</cite> loss estimation subgraph.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.ppo.PPO.process_pc">
<code class="descname">process_pc</code><span class="sig-paren">(</span><em>batch</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/ppo.html#PPO.process_pc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.ppo.PPO.process_pc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns feed dictionary for <cite>pixel control</cite> loss estimation subgraph.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.ppo.PPO.fill_replay_memory">
<code class="descname">fill_replay_memory</code><span class="sig-paren">(</span><em>sess</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/ppo.html#PPO.fill_replay_memory"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.ppo.PPO.fill_replay_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills replay memory with initial experiences.
Supposed to be called by parent worker() just before training begins.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.ppo.PPO.process">
<code class="descname">process</code><span class="sig-paren">(</span><em>sess</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/ppo.html#PPO.process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.ppo.PPO.process" title="Permalink to this definition">¶</a></dt>
<dd><p>Grabs a on_policy_rollout that’s been produced by the thread runner,
samples off_policy rollout[s] from replay memory and updates the parameters.
The update is then sent to the parameter server.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.rollout">
<span id="btgym-algorithms-rollout-module"></span><h2>btgym.algorithms.rollout module<a class="headerlink" href="#module-btgym.algorithms.rollout" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.rollout.Rollout">
<em class="property">class </em><code class="descclassname">btgym.algorithms.rollout.</code><code class="descname">Rollout</code><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout" title="Permalink to this definition">¶</a></dt>
<dd><p>Experience rollout as [nested] dictionary of lists.</p>
<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>values_dict</em>, <em>_dict=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds single experience to rollout by appending values to dictionary of lists.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>values_dict</strong> – [nested] dictionary of values.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.add_memory_sample">
<code class="descname">add_memory_sample</code><span class="sig-paren">(</span><em>sample</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.add_memory_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.add_memory_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Given replay memory sample as list of experience-dictionaries of <cite>length</cite>,
converts it to rollout of same <cite>length</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.process">
<code class="descname">process</code><span class="sig-paren">(</span><em>gamma</em>, <em>gae_lambda=1.0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.process" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts rollout to dictionary of ready-to-feed arrays.
Computes rollout returns and the advantages.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">batch as [nested] dictionary.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.extract">
<code class="descname">extract</code><span class="sig-paren">(</span><em>idx</em>, <em>_struct=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.extract"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.extract" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts single experience from rollout.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>idx</strong> – experience position</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">[nested] dictionary</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.unreal">
<span id="btgym-algorithms-unreal-module"></span><h2>btgym.algorithms.unreal module<a class="headerlink" href="#module-btgym.algorithms.unreal" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.unreal.Unreal">
<em class="property">class </em><code class="descclassname">btgym.algorithms.unreal.</code><code class="descname">Unreal</code><span class="sig-paren">(</span><em>env</em>, <em>task</em>, <em>policy_class</em>, <em>policy_config</em>, <em>log</em>, <em>random_seed=None</em>, <em>model_gamma=0.99</em>, <em>model_gae_lambda=1.0</em>, <em>model_beta=0.01</em>, <em>opt_max_train_steps=10000000</em>, <em>opt_decay_steps=None</em>, <em>opt_end_learn_rate=None</em>, <em>opt_learn_rate=0.0001</em>, <em>opt_decay=0.99</em>, <em>opt_momentum=0.0</em>, <em>opt_epsilon=1e-10</em>, <em>rollout_length=20</em>, <em>episode_summary_freq=2</em>, <em>env_render_freq=10</em>, <em>model_summary_freq=100</em>, <em>test_mode=False</em>, <em>replay_memory_size=2000</em>, <em>replay_rollout_length=None</em>, <em>use_off_policy_aac=False</em>, <em>use_reward_prediction=False</em>, <em>use_pixel_control=False</em>, <em>use_value_replay=False</em>, <em>use_rebalanced_replay=False</em>, <em>rebalance_skewness=2</em>, <em>rp_lambda=1</em>, <em>pc_lambda=0.1</em>, <em>vr_lambda=1</em>, <em>off_aac_lambda=1</em>, <em>gamma_pc=0.9</em>, <em>rp_reward_threshold=0.1</em>, <em>rp_sequence_size=3</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/unreal.html#Unreal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.unreal.Unreal" title="Permalink to this definition">¶</a></dt>
<dd><p>Asynchronous Advantage Actor Critic with auxiliary control tasks.</p>
<p>This UNREAL implementation borrows heavily from Kosuke Miyoshi code, under Apache License 2.0:
<a class="reference external" href="https://miyosuda.github.io/">https://miyosuda.github.io/</a>
<a class="reference external" href="https://github.com/miyosuda/unreal">https://github.com/miyosuda/unreal</a></p>
<p>Original A3C code comes from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<p>Papers:
<a class="reference external" href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a>
<a class="reference external" href="https://arxiv.org/abs/1611.05397">https://arxiv.org/abs/1611.05397</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – envirionment instance.</li>
<li><strong>task</strong> – int</li>
<li><strong>policy_class</strong> – policy estimator class</li>
<li><strong>policy_config</strong> – config dictionary</li>
<li><strong>log</strong> – parent log</li>
<li><strong>random_seed</strong> – int or None</li>
<li><strong>model_gamma</strong> – gamma discount factor</li>
<li><strong>model_gae_lambda</strong> – GAE lambda</li>
<li><strong>model_beta</strong> – entropy regularization beta</li>
<li><strong>opt_max_train_steps</strong> – train steps to run</li>
<li><strong>opt_decay_steps</strong> – learn ratio decay steps</li>
<li><strong>opt_end_learn_rate</strong> – final lerarn rate</li>
<li><strong>opt_learn_rate</strong> – start learn rate</li>
<li><strong>opt_decay</strong> – optimizer decay, if apll.</li>
<li><strong>opt_momentum</strong> – optimizer momentum, if apll.</li>
<li><strong>opt_epsilon</strong> – optimizer epsilon</li>
<li><strong>rollout_length</strong> – on-policy rollout length</li>
<li><strong>episode_summary_freq</strong> – int, write episode summary for every i’th episode</li>
<li><strong>env_render_freq</strong> – int, write environment rendering summary for every i’th train step</li>
<li><strong>model_summary_freq</strong> – int, write model summary for every i’th train step</li>
<li><strong>test_mode</strong> – True: Atari, False: BTGym</li>
<li><strong>replay_memory_size</strong> – in number of experiences</li>
<li><strong>replay_rollout_length</strong> – off-policy rollout length</li>
<li><strong>use_off_policy_aac</strong> – use full AAC off policy training instead of Value-replay</li>
<li><strong>use_reward_prediction</strong> – use aux. off-policy reward prediction task</li>
<li><strong>use_pixel_control</strong> – use aux. off-policy pixel control task</li>
<li><strong>use_value_replay</strong> – use aux. off-policy value replay task (not used, if use_off_policy_aac=True)</li>
<li><strong>use_rebalanced_replay</strong> – NOT USED</li>
<li><strong>rebalance_skewness</strong> – NOT USED</li>
<li><strong>rp_lambda</strong> – reward prediction loss weight</li>
<li><strong>pc_lambda</strong> – pixel control loss weight</li>
<li><strong>vr_lambda</strong> – value replay loss weight</li>
<li><strong>off_aac_lambda</strong> – off-policy AAC loss weight</li>
<li><strong>gamma_pc</strong> – NOT USED</li>
<li><strong>rp_reward_threshold</strong> – reward prediction task classification threshold, above which reward is ‘non-zero’</li>
<li><strong>rp_sequence_size</strong> – reward prediction sample size, in number of experiences</li>
<li><strong>**kwargs</strong> – NOT USED</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.unreal.Unreal.pull_batch_from_queue">
<code class="descname">pull_batch_from_queue</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/unreal.html#Unreal.pull_batch_from_queue"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.unreal.Unreal.pull_batch_from_queue" title="Permalink to this definition">¶</a></dt>
<dd><p>Self explanatory:  take a rollout from the queue of the thread runner.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.unreal.Unreal.process_rp">
<code class="descname">process_rp</code><span class="sig-paren">(</span><em>rp_experience_frames</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/unreal.html#Unreal.process_rp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.unreal.Unreal.process_rp" title="Permalink to this definition">¶</a></dt>
<dd><p>Estimates reward prediction target.
Returns feed dictionary for <cite>reward prediction</cite> loss estimation subgraph.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.unreal.Unreal.process_vr">
<code class="descname">process_vr</code><span class="sig-paren">(</span><em>batch</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/unreal.html#Unreal.process_vr"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.unreal.Unreal.process_vr" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns feed dictionary for <cite>value replay</cite> loss estimation subgraph.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.unreal.Unreal.process_pc">
<code class="descname">process_pc</code><span class="sig-paren">(</span><em>batch</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/unreal.html#Unreal.process_pc"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.unreal.Unreal.process_pc" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns feed dictionary for <cite>pixel control</cite> loss estimation subgraph.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.unreal.Unreal.fill_replay_memory">
<code class="descname">fill_replay_memory</code><span class="sig-paren">(</span><em>sess</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/unreal.html#Unreal.fill_replay_memory"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.unreal.Unreal.fill_replay_memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills replay memory with initial experiences.
Supposed to be called by parent worker() just before training begins.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.unreal.Unreal.process">
<code class="descname">process</code><span class="sig-paren">(</span><em>sess</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/unreal.html#Unreal.process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.unreal.Unreal.process" title="Permalink to this definition">¶</a></dt>
<dd><p>Grabs a on_policy_rollout that’s been produced by the thread runner,
samples off_policy rollout[s] from replay memory and updates the parameters.
The update is then sent to the parameter server.</p>
</dd></dl>

</dd></dl>

</div>
</div>


          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
  <h4>Previous topic</h4>
  <p class="topless"><a href="btgym.rendering.html"
                        title="previous chapter">btgym.rendering package</a></p>
<div id="searchbox" style="display: none" role="search">
  <h3>Quick search</h3>
    <form class="search" action="search.html" method="get">
      <div><input type="text" name="q" /></div>
      <div><input type="submit" value="Go" /></div>
      <input type="hidden" name="check_keywords" value="yes" />
      <input type="hidden" name="area" value="default" />
    </form>
</div>
<script type="text/javascript">$('#searchbox').show(0);</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="related navigation">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="right" >
          <a href="btgym.rendering.html" title="btgym.rendering package"
             >previous</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">BTgym 0.0.6 documentation</a> &#187;</li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
        &#169; Copyright 2017, Andrew Muzikin.
      Created using <a href="http://sphinx-doc.org/">Sphinx</a> 1.6.5.
    </div>
  </body>
</html>