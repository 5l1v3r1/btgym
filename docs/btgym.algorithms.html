

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>btgym.algorithms package &mdash; BTGym 0.0.6 documentation</title>
  

  
  
  
  

  

  
  
    

  

  
  
    <link rel="stylesheet" href="_static/css/theme.css" type="text/css" />
  

  

  
        <link rel="index" title="Index"
              href="genindex.html"/>
        <link rel="search" title="Search" href="search.html"/>
    <link rel="top" title="BTGym 0.0.6 documentation" href="index.html"/>
        <link rel="next" title="btgym.research package" href="btgym.research.html"/>
        <link rel="prev" title="btgym.datafeed package" href="btgym.datafeed.html"/> 

  
  <script src="_static/js/modernizr.min.js"></script>

</head>

<body class="wy-body-for-nav" role="document">

   
  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search">
          

          
            <a href="index.html" class="icon icon-home"> BTGym
          

          
          </a>

          
            
            
              <div class="version">
                0.0.6
              </div>
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <ul>
<li class="toctree-l1"><a class="reference internal" href="intro.html">Package Description</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#installation">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#quickstart">Quickstart</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#environment-engine-description">Environment engine description</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#data-flow-structure">Data flow structure</a></li>
<li class="toctree-l1"><a class="reference internal" href="intro.html#a3c-framework-description">A3C framework description</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.envs.html">btgym.envs package</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.html">btgym.dataserver module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.html#module-btgym.server">btgym.server module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.html#module-btgym.spaces">btgym.spaces module</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.strategy.html">btgym.strategy package</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.monitor.html">btgym.monitor package</a></li>
<li class="toctree-l1"><a class="reference internal" href="btgym.rendering.html">btgym.rendering package</a></li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.datafeed.html">btgym.datafeed package</a></li>
</ul>
<ul class="current">
<li class="toctree-l1 current"><a class="current reference internal" href="#">btgym.algorithms package</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.launcher">btgym.algorithms.launcher module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.worker">btgym.algorithms.worker module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.aac">btgym.algorithms.aac module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.policy">btgym.algorithms.policy module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#btgym-algorithms-losses-module">btgym.algorithms.losses module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#btgym-algorithms-nn-utils-module">btgym.algorithms.nn_utils module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.rollout">btgym.algorithms.rollout module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.memory">btgym.algorithms.memory module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.envs">btgym.algorithms.envs module</a></li>
<li class="toctree-l2"><a class="reference internal" href="#module-btgym.algorithms.runner">btgym.algorithms.runner module</a></li>
</ul>
</li>
</ul>
<ul>
<li class="toctree-l1"><a class="reference internal" href="btgym.research.html">btgym.research package</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">BTGym</a>
        
      </nav>


      
      <div class="wy-nav-content">
        <div class="rst-content">
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="index.html">Docs</a> &raquo;</li>
        
      <li>btgym.algorithms package</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="_sources/btgym.algorithms.rst.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="btgym-algorithms-package">
<h1>btgym.algorithms package<a class="headerlink" href="#btgym-algorithms-package" title="Permalink to this headline">¶</a></h1>
<div class="section" id="module-btgym.algorithms.launcher">
<span id="btgym-algorithms-launcher-module"></span><h2>btgym.algorithms.launcher module<a class="headerlink" href="#module-btgym.algorithms.launcher" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.launcher.Launcher">
<em class="property">class </em><code class="descclassname">btgym.algorithms.launcher.</code><code class="descname">Launcher</code><span class="sig-paren">(</span><em>env_config=None</em>, <em>cluster_config=None</em>, <em>policy_config=None</em>, <em>trainer_config=None</em>, <em>max_env_steps=None</em>, <em>root_random_seed=None</em>, <em>test_mode=False</em>, <em>purge_previous=0</em>, <em>log_level=None</em>, <em>verbose=0</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/launcher.html#Launcher"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.launcher.Launcher" title="Permalink to this definition">¶</a></dt>
<dd><p>Configures and starts distributed TF training session with workers
running sets of separate instances of BTgym/Atari environment.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_config</strong> (<a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)"><em>dict</em></a>) – environment class_config_dict, see ‘Note’ below.</li>
<li><strong>cluster_config</strong> (<a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)"><em>dict</em></a>) – tf cluster configuration, see ‘Note’ below.</li>
<li><strong>policy_config</strong> (<a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)"><em>dict</em></a>) – policy class_config_dict holding corr. policy class args.</li>
<li><strong>trainer_config</strong> (<a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)"><em>dict</em></a>) – trainer class_config_dict holding corr. trainer class args.</li>
<li><strong>max_env_steps</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) – total number of environment steps to run training on.</li>
<li><strong>root_random_seed</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) – int or None</li>
<li><strong>test_mode</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#bool" title="(in Python v2.7)"><em>bool</em></a>) – if True - use Atari gym env., BTGym otherwise.</li>
<li><strong>purge_previous</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) – keep or remove previous log files and saved checkpoints from log_dir:
{0 - keep, 1 - ask, 2 - remove}.</li>
<li><strong>verbose</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) – verbosity mode, {0 - WARNING, 1 - INFO, 2 - DEBUG}.</li>
<li><strong>log_level</strong> (<a class="reference external" href="https://docs.python.org/2/library/functions.html#int" title="(in Python v2.7)"><em>int</em></a>) – logbook level {DEBUG=10, INFO=11, NOTICE=12, WARNING=13},
overrides <cite>verbose</cite> arg.</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<dl class="last docutils">
<dt>class_config_dict:  dictionary containing at least two keys:</dt>
<dd><ul class="first last simple">
<li><cite>class_ref</cite>:    reference to class constructor or function;</li>
<li><cite>kwargs</cite>:       dictionary of keyword arguments, see corr. environment class args.</li>
</ul>
</dd>
<dt>cluster_config:     dictionary containing at least these keys:</dt>
<dd><ul class="first last simple">
<li>‘host’:         cluster host, def: ‘127.0.0.1’</li>
<li>‘port’:         cluster port, def: 12222</li>
<li>‘num_workers’:  number of workers to run, def: 1</li>
<li>‘num_ps’:       number of parameter servers, def: 1</li>
<li>‘num_envs’:     number of environments to run in parallel for each worker, def: 1</li>
<li>‘log_dir’:      directory to save model and summaries, def: ‘./tmp/btgym_aac_log’</li>
</ul>
</dd>
</dl>
</div>
<dl class="method">
<dt id="btgym.algorithms.launcher.Launcher.make_cluster_spec">
<code class="descname">make_cluster_spec</code><span class="sig-paren">(</span><em>config</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/launcher.html#Launcher.make_cluster_spec"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.launcher.Launcher.make_cluster_spec" title="Permalink to this definition">¶</a></dt>
<dd><p>Composes cluster specification dictionary.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.launcher.Launcher.clear_port">
<code class="descname">clear_port</code><span class="sig-paren">(</span><em>port_list</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/launcher.html#Launcher.clear_port"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.launcher.Launcher.clear_port" title="Permalink to this definition">¶</a></dt>
<dd><p>Kills process on specified ports list, if any.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.launcher.Launcher.run">
<code class="descname">run</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/launcher.html#Launcher.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.launcher.Launcher.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Launches processes:</p>
<blockquote>
<div>distributed workers;
parameter_server.</div></blockquote>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.worker">
<span id="btgym-algorithms-worker-module"></span><h2>btgym.algorithms.worker module<a class="headerlink" href="#module-btgym.algorithms.worker" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.worker.Worker">
<em class="property">class </em><code class="descclassname">btgym.algorithms.worker.</code><code class="descname">Worker</code><span class="sig-paren">(</span><em>env_config</em>, <em>policy_config</em>, <em>trainer_config</em>, <em>cluster_spec</em>, <em>job_name</em>, <em>task</em>, <em>log_dir</em>, <em>log_level</em>, <em>max_env_steps</em>, <em>random_seed=None</em>, <em>test_mode=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/worker.html#Worker"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.worker.Worker" title="Permalink to this definition">¶</a></dt>
<dd><p>Distributed tf worker class.</p>
<p>Sets up environment, trainer and starts training process in supervised session.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env_config</strong> – environment class_config_dict.</li>
<li><strong>policy_config</strong> – model policy estimator class_config_dict.</li>
<li><strong>trainer_config</strong> – algorithm class_config_dict.</li>
<li><strong>cluster_spec</strong> – tf.cluster specification.</li>
<li><strong>job_name</strong> – worker or parameter server.</li>
<li><strong>task</strong> – integer number, 0 is chief worker.</li>
<li><strong>log_dir</strong> – for tb summaries and checkpoints.</li>
<li><strong>log_level</strong> – int, logbook.level</li>
<li><strong>max_env_steps</strong> – number of environment steps to run training on</li>
<li><strong>test_mode</strong> – if True - use Atari mode, BTGym otherwise.</li>
<li><strong>Note</strong> – <ul>
<li><dl class="first docutils">
<dt>Conventional <cite>self.global_step</cite> refers to number of environment steps,</dt>
<dd>summarized over all environment instances, not to number of policy optimizer train steps.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Every worker can run several environments in parralell, as specified by <a href="#id1"><span class="problematic" id="id2">`</span></a>cluster_config’[‘num_envs’].</dt>
<dd>If use 4 forkers and num_envs=4 =&gt; total number of environments is 16. Every env instance has
it’s own ThreadRunner process.</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>When using replay memory, keep in mind that every ThreadRunner is keeping it’s own replay memory,</dt>
<dd>If memory_size = 2000, num_workers=4, num_envs=4 =&gt; total replay memory size equals 32 000 frames.</dd>
</dl>
</li>
</ul>
</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.worker.Worker.run">
<code class="descname">run</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/worker.html#Worker.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.worker.Worker.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Worker runtime body.</p>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.aac">
<span id="btgym-algorithms-aac-module"></span><h2>btgym.algorithms.aac module<a class="headerlink" href="#module-btgym.algorithms.aac" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.aac.BaseAAC">
<em class="property">class </em><code class="descclassname">btgym.algorithms.aac.</code><code class="descname">BaseAAC</code><span class="sig-paren">(</span><em>env</em>, <em>task</em>, <em>policy_config</em>, <em>log_level</em>, <em>_log_name='AAC'</em>, <em>on_policy_loss=&lt;function aac_loss_def&gt;</em>, <em>off_policy_loss=&lt;function aac_loss_def&gt;</em>, <em>vr_loss=&lt;function value_fn_loss_def&gt;</em>, <em>rp_loss=&lt;function rp_loss_def&gt;</em>, <em>pc_loss=&lt;function pc_loss_def&gt;</em>, <em>random_seed=None</em>, <em>model_gamma=0.99</em>, <em>model_gae_lambda=1.0</em>, <em>model_beta=0.01</em>, <em>opt_max_env_steps=10000000</em>, <em>opt_decay_steps=None</em>, <em>opt_end_learn_rate=None</em>, <em>opt_learn_rate=0.0001</em>, <em>opt_decay=0.99</em>, <em>opt_momentum=0.0</em>, <em>opt_epsilon=1e-08</em>, <em>rollout_length=20</em>, <em>time_flat=False</em>, <em>episode_train_test_cycle=(1</em>, <em>0)</em>, <em>episode_summary_freq=2</em>, <em>env_render_freq=10</em>, <em>model_summary_freq=100</em>, <em>test_mode=False</em>, <em>replay_memory_size=2000</em>, <em>replay_batch_size=None</em>, <em>replay_rollout_length=None</em>, <em>use_off_policy_aac=False</em>, <em>use_reward_prediction=False</em>, <em>use_pixel_control=False</em>, <em>use_value_replay=False</em>, <em>rp_lambda=1.0</em>, <em>pc_lambda=1.0</em>, <em>vr_lambda=1.0</em>, <em>off_aac_lambda=1</em>, <em>gamma_pc=0.9</em>, <em>rp_reward_threshold=0.1</em>, <em>rp_sequence_size=3</em>, <em>clip_epsilon=0.1</em>, <em>num_epochs=1</em>, <em>pi_prime_update_period=1</em>, <em>_use_target_policy=False</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC" title="Permalink to this definition">¶</a></dt>
<dd><p>Base Asynchronous Advantage Actor Critic algorithm framework class with auxiliary control tasks and
option to run several instances of environment for every worker in vectorized fashion, PAAC-like.
Can be configured to run with different losses and policies.</p>
<p>Auxiliary tasks implementation borrows heavily from Kosuke Miyoshi code, under Apache License 2.0:
<a class="reference external" href="https://miyosuda.github.io/">https://miyosuda.github.io/</a>
<a class="reference external" href="https://github.com/miyosuda/unreal">https://github.com/miyosuda/unreal</a></p>
<p>Original A3C code comes from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<p>Papers:
<a class="reference external" href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a>
<a class="reference external" href="https://arxiv.org/abs/1611.05397">https://arxiv.org/abs/1611.05397</a></p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – environment instance or list of instances</li>
<li><strong>task</strong> – int, parent worker id</li>
<li><strong>policy_config</strong> – policy estimator class and configuration dictionary</li>
<li><strong>log_level</strong> – int, logbook.level</li>
<li><strong>_log_name</strong> – str, class-wide logger name</li>
<li><strong>on_policy_loss</strong> – callable returning tensor holding on_policy training loss graph and summaries</li>
<li><strong>off_policy_loss</strong> – callable returning tensor holding off_policy training loss graph and summaries</li>
<li><strong>vr_loss</strong> – callable returning tensor holding value replay loss graph and summaries</li>
<li><strong>rp_loss</strong> – callable returning tensor holding reward prediction loss graph and summaries</li>
<li><strong>pc_loss</strong> – callable returning tensor holding pixel_control loss graph and summaries</li>
<li><strong>random_seed</strong> – int or None</li>
<li><strong>model_gamma</strong> – scalar, gamma discount factor</li>
<li><strong>model_gae_lambda</strong> – scalar, GAE lambda</li>
<li><strong>model_beta</strong> – entropy regularization beta, scalar or [high_bound, low_bound] for log_uniform.</li>
<li><strong>opt_max_env_steps</strong> – int, total number of environment steps to run training on.</li>
<li><strong>opt_decay_steps</strong> – int, learn ratio decay steps, in number of environment steps.</li>
<li><strong>opt_end_learn_rate</strong> – scalar, final learn rate</li>
<li><strong>opt_learn_rate</strong> – start learn rate, scalar or [high_bound, low_bound] for log_uniform distr.</li>
<li><strong>opt_decay</strong> – scalar, optimizer decay, if apll.</li>
<li><strong>opt_momentum</strong> – scalar, optimizer momentum, if apll.</li>
<li><strong>opt_epsilon</strong> – scalar, optimizer epsilon</li>
<li><strong>rollout_length</strong> – int, on-policy rollout length</li>
<li><strong>time_flat</strong> – bool, flatten rnn time-steps in rollouts while training - see <cite>Notes</cite> below</li>
<li><strong>episode_train_test_cycle</strong> – tuple or list as (train_number, test_number), def=(1,0): enables infinite
loop such as: run <cite>train_number</cite> of train data episodes,
than <cite>test_number</cite> of test data episodes, repeat. Should be consistent
with provided dataset parameters (test data should exist if <cite>test_number &gt; 0</cite>)</li>
<li><strong>episode_summary_freq</strong> – int, write episode summary for every i’th episode</li>
<li><strong>env_render_freq</strong> – int, write environment rendering summary for every i’th train step</li>
<li><strong>model_summary_freq</strong> – int, write model summary for every i’th train step</li>
<li><strong>test_mode</strong> – bool, True: Atari, False: BTGym</li>
<li><strong>replay_memory_size</strong> – int, in number of experiences</li>
<li><strong>replay_batch_size</strong> – int, mini-batch size for off-policy training, def = 1</li>
<li><strong>replay_rollout_length</strong> – int off-policy rollout length by def. equals on_policy_rollout_length</li>
<li><strong>use_off_policy_aac</strong> – bool, use full AAC off-policy loss instead of Value-replay</li>
<li><strong>use_reward_prediction</strong> – bool, use aux. off-policy reward prediction task</li>
<li><strong>use_pixel_control</strong> – bool, use aux. off-policy pixel control task</li>
<li><strong>use_value_replay</strong> – bool, use aux. off-policy value replay task (not used if use_off_policy_aac=True)</li>
<li><strong>rp_lambda</strong> – reward prediction loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>pc_lambda</strong> – pixel control loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>vr_lambda</strong> – value replay loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>off_aac_lambda</strong> – off-policy AAC loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>gamma_pc</strong> – NOT USED</li>
<li><strong>rp_reward_threshold</strong> – scalar, reward prediction classification threshold, above which reward is ‘non-zero’</li>
<li><strong>rp_sequence_size</strong> – int, reward prediction sample size, in number of experiences</li>
<li><strong>clip_epsilon</strong> – scalar, PPO: surrogate L^clip epsilon</li>
<li><strong>num_epochs</strong> – int, num. of SGD runs for every train step, val. &gt; 1 should be used with caution.</li>
<li><strong>pi_prime_update_period</strong> – int, PPO: pi to pi_old update period in number of train steps, def: 1</li>
<li><strong>_use_target_policy</strong> – bool, PPO: use target policy (aka pi_old), delayed by <cite>pi_prime_update_period</cite> delay</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last">
<li><p class="first">On <cite>time_flat</cite> arg:</p>
<blockquote>
<div><p>There are two alternatives to run RNN part of policy estimator:</p>
<ol class="loweralpha simple">
<li><dl class="first docutils">
<dt>Feed initial RNN state for every experience frame in rollout</dt>
<dd>(those are stored anyway if we want random memory repaly sampling) and do single time-step RNN
advance for all experiences in a batch; this is when time_flat=True;</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Reshape incoming batch after convolution part of network in time-wise fashion</dt>
<dd>for every rollout in a batch i.e. batch_size=number_of_rollouts and
rnn_timesteps=max_rollout_length. In this case we need to feed initial rnn_states
for rollouts only. There is some little extra work to pad rollouts to max_time_size
and feed true rollout lengths to rnn. Thus, when time_flat=False, we unroll RNN in
specified number of time-steps for every rollout.</dd>
</dl>
</li>
</ol>
<p>Both options has pros and cons:</p>
<dl class="docutils">
<dt>Unrolling dynamic RNN is computationally more expensive but gives clearly faster convergence,</dt>
<dd><p class="first last">[possibly] due to the fact that RNN states for 2nd, 3rd, … frames
of rollouts are computed using updated policy estimator, which is supposed to be
closer to optimal one. When time_flattened, every time-step uses RNN states computed
when rollout was collected (i.e. by behavioral policy estimator with older
parameters).</p>
</dd>
<dt>Nevertheless, time_flatting can be interesting</dt>
<dd><p class="first last">because one can safely shuffle training batch or mix on-policy and off-policy data in single mini-batch,
ensuring iid property and allowing, say, proper batch normalisation (this has yet to be tested).</p>
</dd>
</dl>
</div></blockquote>
</li>
</ul>
</div>
<dl class="method">
<dt id="btgym.algorithms.aac.BaseAAC.get_sample_config">
<code class="descname">get_sample_config</code><span class="sig-paren">(</span><em>_new_trial=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC.get_sample_config"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC.get_sample_config" title="Permalink to this definition">¶</a></dt>
<dd><p>Returns environment configuration parameters for next episode to sample.
By default is simple stateful iterator,
works correctly with <cite>DTGymDataset</cite> data class, repeating cycle:</p>
<blockquote>
<div><ul class="simple">
<li>sample <cite>num_train_episodes</cite> from train data,</li>
<li>sample <cite>num_test_episodes</cite> from test data.</li>
</ul>
</div></blockquote>
<p>Convention: supposed to override dummy method of local policy instance, see inside ._make_policy() method</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body">configuration dictionary of type <cite>btgym.datafeed.base.EnvResetConfig</cite></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.aac.BaseAAC.start">
<code class="descname">start</code><span class="sig-paren">(</span><em>sess</em>, <em>summary_writer</em>, <em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC.start"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC.start" title="Permalink to this definition">¶</a></dt>
<dd><p>Executes all initializing operations,
starts environment runner[s].
Supposed to be called by parent worker just before training loop starts.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>sess</strong> – tf session object.</li>
<li><strong>kwargs</strong> – not used by default.</li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.aac.BaseAAC.process_data">
<code class="descname">process_data</code><span class="sig-paren">(</span><em>sess</em>, <em>data</em>, <em>is_train</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC.process_data"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC.process_data" title="Permalink to this definition">¶</a></dt>
<dd><p>Processes data, composes train step feed dictionary.
:param sess: tf session obj.
:param data: data dictionary
:type data: dict
:param is_train: is data provided are train or test</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns:</th><td class="field-body"><strong>feed_dict</strong> – train step feed dictionary</td>
</tr>
<tr class="field-even field"><th class="field-name">Return type:</th><td class="field-body"><a class="reference external" href="https://docs.python.org/2/library/stdtypes.html#dict" title="(in Python v2.7)">dict</a></td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.aac.BaseAAC.process_summary">
<code class="descname">process_summary</code><span class="sig-paren">(</span><em>sess</em>, <em>data</em>, <em>model_data=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC.process_summary"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC.process_summary" title="Permalink to this definition">¶</a></dt>
<dd><p>Fetches and writes summary data from <cite>data</cite> and <cite>model_data</cite>.
:param sess: tf summary obj.
:param data: thread_runner rollouts and metadata
:type data: dict
:param model_data: model summary data</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.aac.BaseAAC.process">
<code class="descname">process</code><span class="sig-paren">(</span><em>sess</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#BaseAAC.process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.BaseAAC.process" title="Permalink to this definition">¶</a></dt>
<dd><p>Grabs an on_policy_rollout [and off_policy rollout[s] from replay memory] that’s been produced
by the thread runner. If data identified as ‘train data’ - computes gradients and updates the parameters;
writes summaries if any. The update is then sent to the parameter server.
If on_policy_rollout identified as ‘test data’ -  no policy update is performed (learn rate is set to zero);
Note that test data does not get stored in replay memory (thread runner area).
Writes all available summaries.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sess</strong> (<em>tensorflow.Session</em>) – tf session obj.</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.aac.Unreal">
<em class="property">class </em><code class="descclassname">btgym.algorithms.aac.</code><code class="descname">Unreal</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#Unreal"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.Unreal" title="Permalink to this definition">¶</a></dt>
<dd><p>Unreal: Asynchronous Advantage Actor Critic with auxiliary control tasks.</p>
<p>Auxiliary tasks implementation borrows heavily from Kosuke Miyoshi code, under Apache License 2.0:
<a class="reference external" href="https://miyosuda.github.io/">https://miyosuda.github.io/</a>
<a class="reference external" href="https://github.com/miyosuda/unreal">https://github.com/miyosuda/unreal</a></p>
<p>Original A3C code comes from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<p>Papers:
<a class="reference external" href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a>
<a class="reference external" href="https://arxiv.org/abs/1611.05397">https://arxiv.org/abs/1611.05397</a></p>
<p>See BaseAAC class args for details:</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – environment instance or list of instances</li>
<li><strong>task</strong> – int, parent worker id</li>
<li><strong>policy_config</strong> – policy estimator class and configuration dictionary</li>
<li><strong>log_level</strong> – int, logbook.level</li>
<li><strong>on_policy_loss</strong> – callable returning tensor holding on_policy training loss graph and summaries</li>
<li><strong>off_policy_loss</strong> – callable returning tensor holding off_policy training loss graph and summaries</li>
<li><strong>vr_loss</strong> – callable returning tensor holding value replay loss graph and summaries</li>
<li><strong>rp_loss</strong> – callable returning tensor holding reward prediction loss graph and summaries</li>
<li><strong>pc_loss</strong> – callable returning tensor holding pixel_control loss graph and summaries</li>
<li><strong>random_seed</strong> – int or None</li>
<li><strong>model_gamma</strong> – scalar, gamma discount factor</li>
<li><strong>model_gae_lambda</strong> – scalar, GAE lambda</li>
<li><strong>model_beta</strong> – entropy regularization beta, scalar or [high_bound, low_bound] for log_uniform.</li>
<li><strong>opt_max_env_steps</strong> – int, total number of environment steps to run training on.</li>
<li><strong>opt_decay_steps</strong> – int, learn ratio decay steps, in number of environment steps.</li>
<li><strong>opt_end_learn_rate</strong> – scalar, final learn rate</li>
<li><strong>opt_learn_rate</strong> – start learn rate, scalar or [high_bound, low_bound] for log_uniform distr.</li>
<li><strong>opt_decay</strong> – scalar, optimizer decay, if apll.</li>
<li><strong>opt_momentum</strong> – scalar, optimizer momentum, if apll.</li>
<li><strong>opt_epsilon</strong> – scalar, optimizer epsilon</li>
<li><strong>rollout_length</strong> – int, on-policy rollout length</li>
<li><strong>time_flat</strong> – bool, flatten rnn time-steps in rollouts while training - see <cite>Notes</cite> below</li>
<li><strong>episode_train_test_cycle</strong> – tuple or list as (train_number, test_number), def=(1,0): enables infinite
loop such as: run <cite>train_number</cite> of train data episodes,
than <cite>test_number</cite> of test data episodes, repeat. Should be consistent
with provided dataset parameters (test data should exist if <cite>test_number &gt; 0</cite>)</li>
<li><strong>episode_summary_freq</strong> – int, write episode summary for every i’th episode</li>
<li><strong>env_render_freq</strong> – int, write environment rendering summary for every i’th train step</li>
<li><strong>model_summary_freq</strong> – int, write model summary for every i’th train step</li>
<li><strong>test_mode</strong> – bool, True: Atari, False: BTGym</li>
<li><strong>replay_memory_size</strong> – int, in number of experiences</li>
<li><strong>replay_batch_size</strong> – int, mini-batch size for off-policy training, def = 1</li>
<li><strong>replay_rollout_length</strong> – int off-policy rollout length by def. equals on_policy_rollout_length</li>
<li><strong>use_off_policy_aac</strong> – bool, use full AAC off-policy loss instead of Value-replay</li>
<li><strong>use_reward_prediction</strong> – bool, use aux. off-policy reward prediction task</li>
<li><strong>use_pixel_control</strong> – bool, use aux. off-policy pixel control task</li>
<li><strong>use_value_replay</strong> – bool, use aux. off-policy value replay task (not used if use_off_policy_aac=True)</li>
<li><strong>rp_lambda</strong> – reward prediction loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>pc_lambda</strong> – pixel control loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>vr_lambda</strong> – value replay loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>off_aac_lambda</strong> – off-policy AAC loss weight, scalar or [high, low] for log_uniform distr.</li>
<li><strong>gamma_pc</strong> – NOT USED</li>
<li><strong>rp_reward_threshold</strong> – scalar, reward prediction classification threshold, above which reward is ‘non-zero’</li>
<li><strong>rp_sequence_size</strong> – int, reward prediction sample size, in number of experiences</li>
<li><strong>clip_epsilon</strong> – scalar, PPO: surrogate L^clip epsilon</li>
<li><strong>num_epochs</strong> – int, num. of SGD runs for every train step, val. &gt; 1 should be used with caution.</li>
<li><strong>pi_prime_update_period</strong> – int, PPO: pi to pi_old update period in number of train steps, def: 1</li>
<li><strong>_use_target_policy</strong> – bool, PPO: use target policy (aka pi_old), delayed by <cite>pi_prime_update_period</cite> delay</li>
</ul>
</td>
</tr>
</tbody>
</table>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<ul class="last">
<li><p class="first">On <cite>time_flat</cite> arg:</p>
<blockquote>
<div><p>There are two alternatives to run RNN part of policy estimator:</p>
<ol class="loweralpha simple">
<li><dl class="first docutils">
<dt>Feed initial RNN state for every experience frame in rollout</dt>
<dd>(those are stored anyway if we want random memory repaly sampling) and do single time-step RNN
advance for all experiences in a batch; this is when time_flat=True;</dd>
</dl>
</li>
<li><dl class="first docutils">
<dt>Reshape incoming batch after convolution part of network in time-wise fashion</dt>
<dd>for every rollout in a batch i.e. batch_size=number_of_rollouts and
rnn_timesteps=max_rollout_length. In this case we need to feed initial rnn_states
for rollouts only. There is some little extra work to pad rollouts to max_time_size
and feed true rollout lengths to rnn. Thus, when time_flat=False, we unroll RNN in
specified number of time-steps for every rollout.</dd>
</dl>
</li>
</ol>
<p>Both options has pros and cons:</p>
<dl class="docutils">
<dt>Unrolling dynamic RNN is computationally more expensive but gives clearly faster convergence,</dt>
<dd><p class="first last">[possibly] due to the fact that RNN states for 2nd, 3rd, … frames
of rollouts are computed using updated policy estimator, which is supposed to be
closer to optimal one. When time_flattened, every time-step uses RNN states computed
when rollout was collected (i.e. by behavioral policy estimator with older
parameters).</p>
</dd>
<dt>Nevertheless, time_flatting can be interesting</dt>
<dd><p class="first last">because one can safely shuffle training batch or mix on-policy and off-policy data in single mini-batch,
ensuring iid property and allowing, say, proper batch normalisation (this has yet to be tested).</p>
</dd>
</dl>
</div></blockquote>
</li>
</ul>
</div>
</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.aac.A3C">
<em class="property">class </em><code class="descclassname">btgym.algorithms.aac.</code><code class="descname">A3C</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#A3C"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.A3C" title="Permalink to this definition">¶</a></dt>
<dd><p>Vanilla Asynchronous Advantage Actor Critic algorithm.</p>
<p>Based on original code taken from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<p>Paper: <a class="reference external" href="https://arxiv.org/abs/1602.01783">https://arxiv.org/abs/1602.01783</a></p>
<p>A3C args. is a subset of BaseAAC arguments, see <cite>BaseAAC</cite> class for descriptions.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – </li>
<li><strong>task</strong> – </li>
<li><strong>policy_config</strong> – </li>
<li><strong>log</strong> – </li>
<li><strong>random_seed</strong> – </li>
<li><strong>model_gamma</strong> – </li>
<li><strong>model_gae_lambda</strong> – </li>
<li><strong>model_beta</strong> – </li>
<li><strong>opt_max_env_steps</strong> – </li>
<li><strong>opt_decay_steps</strong> – </li>
<li><strong>opt_end_learn_rate</strong> – </li>
<li><strong>opt_learn_rate</strong> – </li>
<li><strong>opt_decay</strong> – </li>
<li><strong>opt_momentum</strong> – </li>
<li><strong>opt_epsilon</strong> – </li>
<li><strong>rollout_length</strong> – </li>
<li><strong>episode_summary_freq</strong> – </li>
<li><strong>env_render_freq</strong> – </li>
<li><strong>model_summary_freq</strong> – </li>
<li><strong>test_mode</strong> – </li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.aac.PPO">
<em class="property">class </em><code class="descclassname">btgym.algorithms.aac.</code><code class="descname">PPO</code><span class="sig-paren">(</span><em>**kwargs</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/aac.html#PPO"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.aac.PPO" title="Permalink to this definition">¶</a></dt>
<dd><p>AAC with Proximal Policy Optimization surrogate L^Clip loss,
optionally augmented with auxiliary control tasks.</p>
<p>paper:
<a class="reference external" href="https://arxiv.org/pdf/1707.06347.pdf">https://arxiv.org/pdf/1707.06347.pdf</a></p>
<p>Based on PPO-SGD code from OpenAI <cite>Baselines</cite> repository under MIT licence:
<a class="reference external" href="https://github.com/openai/baselines">https://github.com/openai/baselines</a></p>
<p>Async. framework code comes from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<blockquote>
<div>PPO args. is a subset of BaseAAC arguments, see <cite>BaseAAC</cite> class for descriptions.</div></blockquote>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – </li>
<li><strong>task</strong> – </li>
<li><strong>policy_config</strong> – </li>
<li><strong>log_level</strong> – </li>
<li><strong>vr_loss</strong> – </li>
<li><strong>rp_loss</strong> – </li>
<li><strong>pc_loss</strong> – </li>
<li><strong>random_seed</strong> – </li>
<li><strong>model_gamma</strong> – </li>
<li><strong>model_gae_lambda</strong> – </li>
<li><strong>model_beta</strong> – </li>
<li><strong>opt_max_env_steps</strong> – </li>
<li><strong>opt_decay_steps</strong> – </li>
<li><strong>opt_end_learn_rate</strong> – </li>
<li><strong>opt_learn_rate</strong> – </li>
<li><strong>opt_decay</strong> – </li>
<li><strong>opt_momentum</strong> – </li>
<li><strong>opt_epsilon</strong> – </li>
<li><strong>rollout_length</strong> – </li>
<li><strong>episode_summary_freq</strong> – </li>
<li><strong>env_render_freq</strong> – </li>
<li><strong>model_summary_freq</strong> – </li>
<li><strong>test_mode</strong> – </li>
<li><strong>replay_memory_size</strong> – </li>
<li><strong>replay_rollout_length</strong> – </li>
<li><strong>use_off_policy_aac</strong> – </li>
<li><strong>use_reward_prediction</strong> – </li>
<li><strong>use_pixel_control</strong> – </li>
<li><strong>use_value_replay</strong> – </li>
<li><strong>rp_lambda</strong> – </li>
<li><strong>pc_lambda</strong> – </li>
<li><strong>vr_lambda</strong> – </li>
<li><strong>off_aac_lambda</strong> – </li>
<li><strong>rp_reward_threshold</strong> – </li>
<li><strong>rp_sequence_size</strong> – </li>
<li><strong>clip_epsilon</strong> – </li>
<li><strong>num_epochs</strong> – </li>
<li><strong>pi_prime_update_period</strong> – </li>
</ul>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.policy">
<span id="btgym-algorithms-policy-module"></span><h2>btgym.algorithms.policy module<a class="headerlink" href="#module-btgym.algorithms.policy" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="btgym-algorithms-losses-module">
<h2>btgym.algorithms.losses module<a class="headerlink" href="#btgym-algorithms-losses-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="btgym-algorithms-nn-utils-module">
<h2>btgym.algorithms.nn_utils module<a class="headerlink" href="#btgym-algorithms-nn-utils-module" title="Permalink to this headline">¶</a></h2>
</div>
<div class="section" id="module-btgym.algorithms.rollout">
<span id="btgym-algorithms-rollout-module"></span><h2>btgym.algorithms.rollout module<a class="headerlink" href="#module-btgym.algorithms.rollout" title="Permalink to this headline">¶</a></h2>
<dl class="function">
<dt id="btgym.algorithms.rollout.make_data_getter">
<code class="descclassname">btgym.algorithms.rollout.</code><code class="descname">make_data_getter</code><span class="sig-paren">(</span><em>queue</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#make_data_getter"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.make_data_getter" title="Permalink to this definition">¶</a></dt>
<dd><p>Data stream getter constructor.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>queue</strong> – instance of <cite>Queue</cite> class to get rollouts from.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">callable, returning dictionary of data.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="class">
<dt id="btgym.algorithms.rollout.Rollout">
<em class="property">class </em><code class="descclassname">btgym.algorithms.rollout.</code><code class="descname">Rollout</code><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout" title="Permalink to this definition">¶</a></dt>
<dd><p>Experience rollout as [nested] dictionary of lists of ndarrays, tuples and rnn states.</p>
<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>values</em>, <em>_struct=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds single experience frame to rollout.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>values</strong> – [nested] dictionary of values.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.add_memory_sample">
<code class="descname">add_memory_sample</code><span class="sig-paren">(</span><em>sample</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.add_memory_sample"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.add_memory_sample" title="Permalink to this definition">¶</a></dt>
<dd><p>Given replay memory sample as list of experience-dictionaries of <cite>length</cite>,
converts it to rollout of same <cite>length</cite>.</p>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.process">
<code class="descname">process</code><span class="sig-paren">(</span><em>gamma</em>, <em>gae_lambda=1.0</em>, <em>size=None</em>, <em>time_flat=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.process"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.process" title="Permalink to this definition">¶</a></dt>
<dd><p>Converts single-trajectory rollout of experiences to dictionary of ready-to-feed arrays.
Computes rollout returns and the advantages.
Pads with zeroes to desired length, if size arg is given.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>gamma</strong> – discount factor</li>
<li><strong>gae_lambda</strong> – GAE lambda</li>
<li><strong>size</strong> – if given and time_flat=False, pads outputs with zeroes along <a href="#id3"><span class="problematic" id="id4">`</span></a>time’ dim. to exact ‘size’.</li>
<li><strong>time_flat</strong> – reduce time dimension to 1 step by stacking all experiences along batch dimension.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first"><p>[1, time_size, depth] or [1, size, depth] if not time_flatten and <cite>size</cite> is not/given, with single
<cite>context</cite> entry for entire trajectory, i.e. of size [1, context_depth];</p>
<p>[batch_size, 1, depth], if time_flatten, with batch_size = time_size and <cite>context</cite> entry for
every experience frame, i.e. of size [batch_size, context_depth].</p>
</p>
</td>
</tr>
<tr class="field-odd field"><th class="field-name">Return type:</th><td class="field-body"><p class="first last">batch as [nested] dictionary of np.arrays, tuples and LSTMStateTuples. of size</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.process_rp">
<code class="descname">process_rp</code><span class="sig-paren">(</span><em>reward_threshold=0.1</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.process_rp"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.process_rp" title="Permalink to this definition">¶</a></dt>
<dd><p>Processes rollout process()-alike and estimates reward prediction target for first n-1 frames.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>reward_threshold</strong> – reward values such as <a href="#id5"><span class="problematic" id="id6">|r|</span></a>&gt; reward_threshold are classified as neg. or pos.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">Processed batch with size reduced by one and with extra <cite>rp_target</cite> key
holding one hot encodings for classes {zero, positive, negative}.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.get_frame">
<code class="descname">get_frame</code><span class="sig-paren">(</span><em>idx</em>, <em>_struct=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.get_frame"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.get_frame" title="Permalink to this definition">¶</a></dt>
<dd><p>Extracts single experience from rollout.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>idx</strong> – experience position</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">frame as [nested] dictionary</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.rollout.Rollout.pop_frame">
<code class="descname">pop_frame</code><span class="sig-paren">(</span><em>idx</em>, <em>_struct=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/rollout.html#Rollout.pop_frame"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.rollout.Rollout.pop_frame" title="Permalink to this definition">¶</a></dt>
<dd><p>Pops single experience from rollout.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>idx</strong> – experience position</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">frame as [nested] dictionary</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.memory">
<span id="btgym-algorithms-memory-module"></span><h2>btgym.algorithms.memory module<a class="headerlink" href="#module-btgym.algorithms.memory" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.memory.Memory">
<em class="property">class </em><code class="descclassname">btgym.algorithms.memory.</code><code class="descname">Memory</code><span class="sig-paren">(</span><em>history_size</em>, <em>max_sample_size</em>, <em>priority_sample_size</em>, <em>log_level=13</em>, <em>rollout_provider=None</em>, <em>task=-1</em>, <em>reward_threshold=0.1</em>, <em>use_priority_sampling=False</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory" title="Permalink to this definition">¶</a></dt>
<dd><p>Replay memory with rebalanced replay based on reward value.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">must be filled up before calling sampling methods.</p>
</div>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>history_size</strong> – number of experiences stored;</li>
<li><strong>max_sample_size</strong> – maximum allowed sample size (e.g. off-policy rollout length);</li>
<li><strong>priority_sample_size</strong> – sample size of priority_sample() method</li>
<li><strong>log_level</strong> – int, logbook.level;</li>
<li><strong>rollout_provider</strong> – callable returning list of Rollouts NOT USED</li>
<li><strong>task</strong> – parent worker id;</li>
<li><strong>reward_threshold</strong> – if <a href="#id7"><span class="problematic" id="id8">|experience.reward|</span></a> &gt; reward_threshold: experience is saved as ‘prioritized’;</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.memory.Memory.add">
<code class="descname">add</code><span class="sig-paren">(</span><em>frame</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.add"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.add" title="Permalink to this definition">¶</a></dt>
<dd><p>Appends single experience frame to memory.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>frame</strong> – dictionary of values.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory.add_rollout">
<code class="descname">add_rollout</code><span class="sig-paren">(</span><em>rollout</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.add_rollout"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.add_rollout" title="Permalink to this definition">¶</a></dt>
<dd><p>Adds frames from given rollout to memory with respect to episode continuation.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>rollout</strong> – <cite>Rollout</cite> instance.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory.fill">
<code class="descname">fill</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.fill"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.fill" title="Permalink to this definition">¶</a></dt>
<dd><p>Fills replay memory with initial experiences. NOT USED.
Supposed to be called by parent worker() just before training begins.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>rollout_getter</strong> – callable, returning list of Rollouts.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory.sample_uniform">
<code class="descname">sample_uniform</code><span class="sig-paren">(</span><em>sequence_size</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory.sample_uniform"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory.sample_uniform" title="Permalink to this definition">¶</a></dt>
<dd><p>Uniformly samples sequence of successive frames of size <cite>sequence_size</cite> or less (~off-policy rollout).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>sequence_size</strong> – maximum sample size.</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body">instance of Rollout of size &lt;= sequence_size.</td>
</tr>
</tbody>
</table>
</dd></dl>

<dl class="method">
<dt id="btgym.algorithms.memory.Memory._sample_priority">
<code class="descname">_sample_priority</code><span class="sig-paren">(</span><em>size=None</em>, <em>exact_size=False</em>, <em>skewness=2</em>, <em>sample_attempts=100</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/memory.html#Memory._sample_priority"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.memory.Memory._sample_priority" title="Permalink to this definition">¶</a></dt>
<dd><p>Implements rebalanced replay.
Samples sequence of successive frames from distribution skewed by means of reward of last sample frame.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first simple">
<li><strong>size</strong> – sample size, must be &lt;= self.max_sample_size;</li>
<li><strong>exact_size</strong> – whether accept sample with size less than ‘size’
or re-sample to get sample of exact size (used for reward prediction task);</li>
<li><strong>skewness</strong> – int&gt;=1, sampling probability denominator, such as probability of sampling sequence with
last frame having non-zero reward is: p[non_zero]=1/skewness;</li>
<li><strong>sample_attempts</strong> – if exact_size=True, sets number of re-sampling attempts
to get sample of continuous experiences (no <cite>Terminal</cite> frames inside except last one);
if number is reached - sample returned ‘as is’.</li>
</ul>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns:</th><td class="field-body"><p class="first last">instance of Rollout().</p>
</td>
</tr>
</tbody>
</table>
</dd></dl>

</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.envs">
<span id="btgym-algorithms-envs-module"></span><h2>btgym.algorithms.envs module<a class="headerlink" href="#module-btgym.algorithms.envs" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.envs.AtariRescale42x42">
<em class="property">class </em><code class="descclassname">btgym.algorithms.envs.</code><code class="descname">AtariRescale42x42</code><span class="sig-paren">(</span><em>env_id=None</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/envs.html#AtariRescale42x42"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.envs.AtariRescale42x42" title="Permalink to this definition">¶</a></dt>
<dd><p>Gym wrapper, pipes Atari into BTgym algorithms, as later expect observations to be DictSpace.
Makes Atari environment return state as dictionary with single key ‘external’ holding
normalized in [0,1] grayscale 42x42 visual output.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><strong>env_id</strong> – conventional Gym id.</td>
</tr>
</tbody>
</table>
</dd></dl>

</div>
<div class="section" id="module-btgym.algorithms.runner">
<span id="btgym-algorithms-runner-module"></span><h2>btgym.algorithms.runner module<a class="headerlink" href="#module-btgym.algorithms.runner" title="Permalink to this headline">¶</a></h2>
<dl class="class">
<dt id="btgym.algorithms.runner.RunnerThread">
<em class="property">class </em><code class="descclassname">btgym.algorithms.runner.</code><code class="descname">RunnerThread</code><span class="sig-paren">(</span><em>env</em>, <em>policy</em>, <em>task</em>, <em>rollout_length</em>, <em>episode_summary_freq</em>, <em>env_render_freq</em>, <em>test</em>, <em>ep_summary</em>, <em>memory_config=None</em>, <em>log_level=13</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/runner.html#RunnerThread"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.runner.RunnerThread" title="Permalink to this definition">¶</a></dt>
<dd><p>Async. framework code comes from OpenAI repository under MIT licence:
<a class="reference external" href="https://github.com/openai/universe-starter-agent">https://github.com/openai/universe-starter-agent</a></p>
<p>Despite the fact BTgym is not real-time environment [yet], thread-runner approach is still here. From
original <cite>universe-starter-agent</cite>:
<cite>…One of the key distinctions between a normal environment and a universe environment
is that a universe environment is _real time_.  This means that there should be a thread
that would constantly interact with the environment and tell it what to do.  This thread is here.</cite></p>
<p>Another idea is to see ThreadRunner as all-in-one data provider, thus shaping data distribution
fed to estimator from single place.
So, replay memory is also here, as well as some service functions (collecting summary data).</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – environment instance</li>
<li><strong>policy</strong> – policy instance</li>
<li><strong>task</strong> – int</li>
<li><strong>rollout_length</strong> – int</li>
<li><strong>episode_summary_freq</strong> – int</li>
<li><strong>env_render_freq</strong> – int</li>
<li><strong>test</strong> – Atari or BTGyn</li>
<li><strong>ep_summary</strong> – tf.summary</li>
<li><strong>memory_config</strong> – replay memory configuration dictionary</li>
<li><strong>log_level</strong> – int, logbook.level</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="btgym.algorithms.runner.RunnerThread.run">
<code class="descname">run</code><span class="sig-paren">(</span><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/runner.html#RunnerThread.run"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.runner.RunnerThread.run" title="Permalink to this definition">¶</a></dt>
<dd><p>Just keep running.</p>
</dd></dl>

</dd></dl>

<dl class="function">
<dt id="btgym.algorithms.runner.env_runner">
<code class="descclassname">btgym.algorithms.runner.</code><code class="descname">env_runner</code><span class="sig-paren">(</span><em>sess</em>, <em>env</em>, <em>policy</em>, <em>task</em>, <em>rollout_length</em>, <em>summary_writer</em>, <em>episode_summary_freq</em>, <em>env_render_freq</em>, <em>atari_test</em>, <em>ep_summary</em>, <em>memory_config</em>, <em>log</em><span class="sig-paren">)</span><a class="reference internal" href="_modules/btgym/algorithms/runner.html#env_runner"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#btgym.algorithms.runner.env_runner" title="Permalink to this definition">¶</a></dt>
<dd><p>The logic of the thread runner.
In brief, it constantly keeps on running
the policy, and as long as the rollout exceeds a certain length, the thread
runner appends all the collected data to the queue.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name" />
<col class="field-body" />
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters:</th><td class="field-body"><ul class="first last simple">
<li><strong>env</strong> – environment instance</li>
<li><strong>policy</strong> – policy instance</li>
<li><strong>task</strong> – int</li>
<li><strong>rollout_length</strong> – int</li>
<li><strong>episode_summary_freq</strong> – int</li>
<li><strong>env_render_freq</strong> – int</li>
<li><strong>atari_test</strong> – bool, Atari or BTGyn</li>
<li><strong>ep_summary</strong> – dict of tf.summary op and placeholders</li>
<li><strong>memory_config</strong> – replay memory configuration dictionary</li>
<li><strong>log</strong> – logbook logger</li>
</ul>
</td>
</tr>
</tbody>
</table>
<dl class="docutils">
<dt>Yelds:</dt>
<dd>collected data as dictionary of on_policy, off_policy rollouts and episode statistics.</dd>
</dl>
</dd></dl>

</div>
</div>


           </div>
           <div class="articleComments">
            
           </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="btgym.research.html" class="btn btn-neutral float-right" title="btgym.research package" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right"></span></a>
      
      
        <a href="btgym.datafeed.html" class="btn btn-neutral" title="btgym.datafeed package" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2017, Andrew Muzikin.

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  

    <script type="text/javascript">
        var DOCUMENTATION_OPTIONS = {
            URL_ROOT:'./',
            VERSION:'0.0.6',
            COLLAPSE_INDEX:false,
            FILE_SUFFIX:'.html',
            HAS_SOURCE:  true,
            SOURCELINK_SUFFIX: '.txt'
        };
    </script>
      <script type="text/javascript" src="_static/jquery.js"></script>
      <script type="text/javascript" src="_static/underscore.js"></script>
      <script type="text/javascript" src="_static/doctools.js"></script>
      <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

  

  
  
    <script type="text/javascript" src="_static/js/theme.js"></script>
  

  
  
  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.StickyNav.enable();
      });
  </script>
   

</body>
</html>