{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import backtrader.indicators as btind\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "from scipy import stats\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "from btgym import BTgymEnv, BTgymStrategy, BTgymDataset\n",
    "\n",
    "from btgym.a3c import Launcher, LSTMPolicy, BaseLSTMPolicy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "\n",
    "class LSTMPolicyTFF(BaseLSTMPolicy):\n",
    "    \"\"\"\n",
    "    Time/frequency LSTM feature extraction.\n",
    "    \"\"\"\n",
    "    def __init__(self, ob_space, ac_space,\n",
    "                 lstm_class=rnn.BasicLSTMCell, lstm_layers=(256, 256)):\n",
    "        \n",
    "        # Run LSTM along time-embedding dim:\n",
    "        self.x = x = x_f = tf.placeholder(tf.float32, [None] + list(ob_space), name='x_in_pl')\n",
    "\n",
    "        lstm_t = lstm_class(128, state_is_tuple=True,)\n",
    "\n",
    "        num_time_steps = tf.expand_dims(tf.shape(x)[1], [0])\n",
    "        n_t_expanded = tf.fill(tf.expand_dims(tf.shape(x)[0], [0]), num_time_steps[0])\n",
    "        batch_size = tf.shape(x)[0]\n",
    "\n",
    "        lstm_t_outputs, lstm_t_state = tf.nn.dynamic_rnn(\n",
    "            lstm_t,\n",
    "            x,\n",
    "            initial_state=lstm_t.zero_state(batch_size, dtype=tf.float32),\n",
    "            sequence_length=n_t_expanded,\n",
    "            time_major=False,\n",
    "            scope='time_embed_lstm')\n",
    "        \n",
    "        # Run LSTM along frequency dimension:\n",
    "        x_f = tf.transpose(x_f, perm=[0,2,1])\n",
    "\n",
    "        lstm_f =  lstm_class(128, state_is_tuple=True,)\n",
    "        num_time_steps = tf.expand_dims(tf.shape(x_f)[1], [0])\n",
    "        n_t_expanded = tf.fill(tf.expand_dims(tf.shape(x_f)[0], [0]), num_time_steps[0])\n",
    "        batch_size = tf.shape(x_f)[0]\n",
    "        \n",
    "        lstm_f_outputs, lstm_f_state = tf.nn.dynamic_rnn(\n",
    "            lstm_f,\n",
    "            x_f,\n",
    "            initial_state=lstm_f.zero_state(batch_size, dtype=tf.float32),\n",
    "            sequence_length=n_t_expanded,\n",
    "            time_major=False,\n",
    "            scope='frequency_embed_lstm')\n",
    "    \n",
    "        x_feature = tf.concat([lstm_t_state.h, lstm_f_state.h], 1)\n",
    "        \n",
    "        # Run LSTM along rollout time dimension and evrything else:\n",
    "        super(MultiLSTMPolicy, self).__init__(x_feature, ob_space, ac_space, lstm_class, lstm_layers)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyStrategy(BTgymStrategy):\n",
    "    \"\"\"\n",
    "    Example subclass of BT server inner computation startegy.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyStrategy,self).__init__(**kwargs)\n",
    "        self.order_penalty = 1\n",
    "        self.trade_just_closed = False\n",
    "        self.trade_result = None\n",
    "        \n",
    "    def notify_trade(self, trade):    \n",
    "        if trade.isclosed:\n",
    "            # Set trade flag and result:\n",
    "            self.trade_just_closed = True\n",
    "            self.trade_result = trade.pnlcomm\n",
    "        \n",
    "    def get_state(self):\n",
    "        \"\"\"\n",
    "        Computes featurized RL-ready environment observation state\n",
    "        by applying continious wavelet transform to time-embedded vector\n",
    "        of close-price gradients.\n",
    "        \"\"\"\n",
    "        # Use close price:\n",
    "        channel = 3\n",
    "        X = self.raw_state[:, channel]\n",
    "        \n",
    "        # Prepare parameters:\n",
    "        Tau = 2\n",
    "        max_cwt_scale = self.p.state_shape['model_input'].shape[1] #- 1\n",
    "        cwt_width = np.linspace(Tau, max_cwt_scale + Tau - 1, max_cwt_scale) # scale of wavelet transdorm [n]\n",
    "    \n",
    "        T = 1e4\n",
    "        \n",
    "        # Get amplified vector of gradients of last [n] prices:\n",
    "        X = np.gradient(X, axis=0) * T\n",
    "        \n",
    "        # Compute continious wavelet transform using Ricker wavelet, get [n,m,1]-dim. matrix:\n",
    "        X = signal.cwt(X, signal.ricker, cwt_width).T\n",
    "        \n",
    "        self.state['model_input'] = X \n",
    "    \n",
    "        return self.state\n",
    "    \n",
    "    def get_reward(self):\n",
    "        \"\"\"\n",
    "        Defines reward as function of last closed trade result with penalty for erronious order placement.\n",
    "        \"\"\"\n",
    "        # Low-value term:\n",
    "        r = (self.broker.get_value() / self.env.broker.startingcash - 1) * 10\n",
    "        \n",
    "        # Result (main-value):\n",
    "        if self.trade_just_closed:\n",
    "            r += self.trade_result\n",
    "            self.trade_just_closed = False\n",
    "            \n",
    "        # Penalty for failed order:\n",
    "        if self.order_failed:\n",
    "            r -= self.order_penalty\n",
    "            self.order_failed = False\n",
    "\n",
    "        return r / 20  # sinse reward is just quick-example, denominator is here to keep gradients sane\n",
    "\n",
    "# Set backtesting engine parameters:\n",
    "\n",
    "time_embed_dim = 30\n",
    "\n",
    "state_shape = {\n",
    "    'raw_state': spaces.Box(low=-1, high=1, shape=(time_embed_dim, 4)),\n",
    "    'model_input': spaces.Box(low=-10, high=10, shape=(time_embed_dim, 15))\n",
    "}\n",
    "\n",
    "MyCerebro = bt.Cerebro()\n",
    "\n",
    "MyCerebro.addstrategy(\n",
    "    MyStrategy,\n",
    "    state_shape=state_shape,\n",
    "    portfolio_actions=('hold', 'buy', 'sell'),\n",
    "    drawdown_call=5, # max to loose, in percent of initial cash\n",
    "    target_call=10,  # max to win, same\n",
    "    skip_frame=10,\n",
    ")\n",
    "\n",
    "# Set leveraged account:\n",
    "MyCerebro.broker.setcash(2000)\n",
    "MyCerebro.broker.setcommission(commission=0.0001, leverage=10.0) # commisssion to imitate spread\n",
    "MyCerebro.broker.set_shortcash(False)\n",
    "MyCerebro.addsizer(bt.sizers.SizerFix, stake=10000,)\n",
    "\n",
    "MyCerebro.addanalyzer(bt.analyzers.DrawDown)\n",
    "\n",
    "MyDataset = BTgymDataset(\n",
    "    #filename='../examples/data/DAT_ASCII_EURUSD_M1_2016.csv',\n",
    "    filename='../examples/data/test_sine_1min_period256_delta0002.csv',\n",
    "    start_weekdays=[0, 1, 2, 3, 4],\n",
    "    episode_len_days=1,\n",
    "    episode_len_hours=23,\n",
    "    episode_len_minutes=0,\n",
    "    start_00=False,\n",
    "    time_gap_hours=2,\n",
    ")\n",
    "env_config = dict(\n",
    "    dataset=MyDataset,\n",
    "    engine=MyCerebro,\n",
    "    render_modes=['episode', 'human', 'model_input'],\n",
    "    render_state_as_image=True,\n",
    "    render_ylabel='OHLC Gradients',\n",
    "    render_size_episode=(12,8),\n",
    "    render_size_human=(8, 3.5),\n",
    "    render_size_state=(10, 5),\n",
    "    render_dpi=75,\n",
    "    port=5000,\n",
    "    data_port=4999,\n",
    "    connect_timeout=60,\n",
    "    verbose=0,\n",
    ")\n",
    "\n",
    "# Set tensorflow distributed cluster and a3c configuration:\n",
    "cluster_config = dict(\n",
    "    host='127.0.0.1',\n",
    "    port=12222,\n",
    "    num_workers=8,\n",
    "    num_ps=1,\n",
    "    log_dir='./tmp/a3c_test_tf_2016',\n",
    ")\n",
    "launcher = Launcher(\n",
    "    cluster_config=cluster_config,\n",
    "    env_class=BTgymEnv,\n",
    "    env_config=env_config,\n",
    "    policy_class=MultiLSTMPolicy,\n",
    "    policy_config={'lstm_layers': (256, 256)},\n",
    "    rollout_length=20,\n",
    "    test_mode=False,\n",
    "    train_steps=1000000000,\n",
    "    model_summary_freq=20,\n",
    "    episode_summary_freq=1,\n",
    "    env_render_freq=10,\n",
    "    verbose=2\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "launcher.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env_config.update({'port': 5050, 'data_port': 5049})\n",
    "env = BTgymEnv(**env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o,r,d,i = env.step(0)\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
