{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small real dataset test.\n",
    "---------\n",
    "###### A3C tweaked to ~~solve~~ overfit on dataset of 1 month of EURUSD 1-minute bar data in ~ 10M iterations. \n",
    "\n",
    "Key points:\n",
    "- using broker/portfolio statistics to augument state observation ('inner' state);\n",
    "- using 1d convolutions along time-embedding dimension for 'outer' state signal;\n",
    "- shaping outer observation state as bank of open-price simple moving averages with increasing time-periods;\n",
    "- ensuring signal stationarity by taking gradients along features axis and subsequent log-scaling;\n",
    "- feeding 'inner' part of state directly to LSTM layer;\n",
    "- shaping reward as weighted sum of averaged broker statisitcs: unrealised profit/loss, realized trade result, current portfolio value;\n",
    "- allowing variable trade position size;\n",
    "- using env.skip_frame=10. Introducing prior actions distribution seems critical to agent performance. I wasn't been able to acheve good results with skip_frame < 8. Not a proper way to shape priors, but working for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import backtrader.indicators as btind\n",
    "import numpy as np\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "from btgym import BTgymEnv, BTgymStrategy, BTgymDataset\n",
    "\n",
    "from btgym.a3c import Launcher, BaseLSTMPolicy\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "from tensorflow.contrib.layers import flatten as batch_flatten\n",
    "from tensorflow.python.util.nest import flatten as flatten_nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMPolicy1D(BaseLSTMPolicy):\n",
    "    \"\"\"\n",
    "    A3C LSTM policy with 1D convolutions.\n",
    "    \"\"\"\n",
    "    def __init__(self, ob_space, ac_space, num_filters=32, filter_size=5, stride=2,\n",
    "                 lstm_class=rnn.BasicLSTMCell, lstm_layers=(256,)):\n",
    "        \n",
    "        self.x = x = tf.placeholder(tf.float32, [None] + list(ob_space), name='x_in_pl')\n",
    "        \n",
    "        # Outer state:\n",
    "        x = x[..., :8]\n",
    "        \n",
    "        # Inner state:\n",
    "        x2 = x[..., -3:]\n",
    "\n",
    "        # Pass outer state through conv layers:\n",
    "        for i in range(4):\n",
    "            x = tf.nn.elu(self.conv1d(x, num_filters, \"l{}\".format(i + 1), filter_size, stride))\n",
    "            \n",
    "        x = tf.concat(\n",
    "            [batch_flatten(x), batch_flatten(x2)],\n",
    "            axis=-1\n",
    "        )\n",
    "      \n",
    "        # Run LSTM along rollout time dimension and evrything else:\n",
    "        super(LSTMPolicy1D, self).__init__(x, ob_space, ac_space, lstm_class, lstm_layers)\n",
    "           \n",
    "    def conv1d(self, x, num_filters, name, filter_size=3, stride=2, pad=\"SAME\", dtype=tf.float32,\n",
    "               collections=None):\n",
    "        with tf.variable_scope(name):\n",
    "            stride_shape =  stride\n",
    "            \n",
    "            #print('stride_shape:',stride_shape)\n",
    "            \n",
    "            filter_shape = [filter_size, int(x.get_shape()[-1]), num_filters]\n",
    "            \n",
    "            #print('filter_shape:', filter_shape)\n",
    "            \n",
    "            # there are \"num input feature maps * filter height * filter width\"\n",
    "            # inputs to each hidden unit\n",
    "            fan_in = np.prod(filter_shape[:2])\n",
    "            \n",
    "            # each unit in the lower layer receives a gradient from:\n",
    "            # \"num output feature maps * filter height * filter width\" /\n",
    "            #   pooling size\n",
    "            fan_out = np.prod(filter_shape[:1]) * num_filters\n",
    "\n",
    "            # initialize weights with random weights\n",
    "            w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "\n",
    "            w = tf.get_variable(\"W\", filter_shape, dtype, tf.random_uniform_initializer(-w_bound, w_bound),\n",
    "                                collections=collections)\n",
    "            b = tf.get_variable(\"b\", [1, 1, num_filters], initializer=tf.constant_initializer(0.0),\n",
    "                                collections=collections)\n",
    "            return tf.nn.conv1d(x, w, stride_shape, pad) + b\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyStrategy(BTgymStrategy):\n",
    "    \"\"\"\n",
    "    BT server inner computation startegy tuned to pass small real data test.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyStrategy, self).__init__(**kwargs)\n",
    "        \n",
    "        self.dim_time = self.p.state_shape['raw_state'].shape[0] \n",
    "\n",
    "        self.trade_just_closed = False\n",
    "        self.trade_result = None\n",
    "        \n",
    "        self.unrealized_pnl = None\n",
    "        self.norm_broker_value = None\n",
    "        self.avg_norm_broker_cash = 0 # not used\n",
    "        self.realized_pnl = None\n",
    "\n",
    "        self.realized_broker_value = self.env.broker.startingcash\n",
    "        self.episode_result = 0  # not used\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.avg_period = 10 # not used\n",
    "        \n",
    "        # Z-normalization over entire dataset of O-prices: \n",
    "        #mean = np.average(self.p.dataset_stat[1:3].values[0,:-1])\n",
    "        #std = np.average(self.p.dataset_stat[1:3].values[1,:-1])\n",
    "        #self.x = (self.datas[0] - mean) / std \n",
    "        \n",
    "        self.x = self.datas[0] # using O-price as signal. TODO: subopt, H/L avg?\n",
    "        \n",
    "        # Signal features:\n",
    "        self.data.sma_4 = btind.SimpleMovingAverage(self.x, period=4)\n",
    "        self.data.sma_8 = btind.SimpleMovingAverage(self.x, period=8)\n",
    "        self.data.sma_16 = btind.SimpleMovingAverage(self.x, period=16)\n",
    "        self.data.sma_32 = btind.SimpleMovingAverage(self.x, period=32)\n",
    "        self.data.sma_64 = btind.SimpleMovingAverage(self.x, period=64)\n",
    "        self.data.sma_128 = btind.SimpleMovingAverage(self.x, period=128)\n",
    "        self.data.sma_256 = btind.SimpleMovingAverage(self.x, period=256) \n",
    "\n",
    "        # Service sma to get correct first features values:\n",
    "        self.data.dim_sma = btind.SimpleMovingAverage(\n",
    "            self.datas[0],\n",
    "            period=(256 + self.dim_time)\n",
    "        )\n",
    "        self.data.dim_sma.plotinfo.plot = False\n",
    "        \n",
    "    def notify_trade(self, trade):    \n",
    "        if trade.isclosed:\n",
    "            # Set trade flag [True if trade have been closed within last frame-skip period] \n",
    "            # and store trade result:\n",
    "            self.trade_just_closed = True\n",
    "            self.trade_result = trade.pnlcomm\n",
    "            \n",
    "            # Store realized prtfolio value:\n",
    "            self.realized_broker_value = self.broker.get_value()\n",
    " \n",
    "    def get_state(self):\n",
    "        \"\"\" \n",
    "        Computes obs. state as [time_dim, 11] matrix of log-scaled signal features gradients\n",
    "        and portfolio statistics.\n",
    "        \"\"\"\n",
    "        T = 1e4 # EURUSD\n",
    "        #T = 1e2 # EURUSD, Z-norm\n",
    "        #T = 1 # BTCUSD\n",
    "      \n",
    "        x = np.stack(\n",
    "            [\n",
    "                np.frombuffer(self.x.get(size=self.dim_time)),\n",
    "                np.frombuffer(self.data.sma_4.get(size=self.dim_time)), \n",
    "                np.frombuffer(self.data.sma_8.get(size=self.dim_time)), \n",
    "                np.frombuffer(self.data.sma_16.get(size=self.dim_time)), \n",
    "                np.frombuffer(self.data.sma_32.get(size=self.dim_time)),\n",
    "                np.frombuffer(self.data.sma_64.get(size=self.dim_time)),\n",
    "                np.frombuffer(self.data.sma_128.get(size=self.dim_time)),\n",
    "                np.frombuffer(self.data.sma_256.get(size=self.dim_time)), \n",
    "            ], \n",
    "            axis=-1\n",
    "        )\n",
    "        \n",
    "        # Amplified gradient along features axis:\n",
    "        x = np.gradient(x, axis=1) * T\n",
    "        \n",
    "        # Log-scale:\n",
    "        x = self.log_transform(x)\n",
    "        \n",
    "        # Add inner state statistic:\n",
    "        x = np.concatenate(\n",
    "            [\n",
    "                x,\n",
    "                self.get_unrealized_pnl(size=self.dim_time)[..., None],\n",
    "                self.get_norm_broker_value(size=self.dim_time)[..., None],\n",
    "                self.get_realized_pnl(size=self.dim_time, gamma=1.0)[..., None],\n",
    "            ],\n",
    "            axis=-1\n",
    "        )\n",
    "        self.state['raw_state'] = self.raw_state\n",
    "                \n",
    "        self.state['model_input'] = x\n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "    def log_transform(self, x):\n",
    "        return np.sign(x) * np.log(np.fabs(x) + 1)\n",
    "\n",
    "    def norm_log_value(self, current_value, start_value, drawdown_call, target_call, epsilon=1e-4):\n",
    "        \"\"\"Current value log-normalized in [-1,1] wrt upper and lower bounds\"\"\"\n",
    "        x = np.asarray(current_value)\n",
    "        x = (x / start_value - 1) * 100\n",
    "        x = (x - target_call)/(drawdown_call+target_call) + 1\n",
    "        x = np.clip(x, epsilon, 1 - epsilon)\n",
    "        x = 1 - 2 * np.log(x) / np.log( epsilon)\n",
    "        return x\n",
    "    \n",
    "    def norm_value(self, current_value, start_value, drawdown_call, target_call, epsilon=1e-8):\n",
    "        \"\"\"Current value normalized in [-1,1] wrt upper and lower bounds\"\"\"\n",
    "        x = np.asarray(current_value)\n",
    "        x = (x / start_value - 1) * 100\n",
    "        x = (x - target_call)/(drawdown_call+target_call) + 1\n",
    "        x = 2 * np.clip(x, epsilon, 1 - epsilon) - 1 \n",
    "        return x\n",
    "    \n",
    "    def decayed_result(self, trade_result, current_value, start_value, drawdown_call, target_call, gamma=1.0):\n",
    "        \"Normalized in [-1,1] trade result, lineary decayed wrt current value.\"\n",
    "        target_value = start_value * (1 + target_call/100)\n",
    "        value_range = start_value * (drawdown_call + target_call)/100\n",
    "        decay = (gamma - 1) * (current_value - target_value) / value_range + gamma\n",
    "        x = trade_result * decay / value_range\n",
    "        return x\n",
    "    \n",
    "    def get_unrealized_pnl(self, size=1):\n",
    "        \"\"\"\n",
    "        Normalized profit/loss for current opened trade (unrealized p/l).\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\"\n",
    "        x = np.asarray(self.stats.broker.value.get(size=size)) - self.realized_broker_value\n",
    "        self.unrealized_pnl = x / self.env.broker.startingcash /(self.p.drawdown_call + self.p.target_call) * 100\n",
    "        return self.unrealized_pnl\n",
    "    \n",
    "    def get_realized_pnl(self, size=1, gamma=1.0):\n",
    "        \"\"\"\n",
    "        Returns single trade realized profit/loss, normalized and gamma-ajusted to current broker value.\n",
    "        Returns vector of filled with p/l value of length `size`.\n",
    "        \"\"\"\n",
    "        if self.trade_just_closed:\n",
    "            self.realized_pnl = self.decayed_result(\n",
    "                self.trade_result,\n",
    "                np.asarray(self.stats.broker.value.get(size=size)),\n",
    "                self.env.broker.startingcash,\n",
    "                self.p.drawdown_call,\n",
    "                self.p.target_call,\n",
    "                gamma=gamma\n",
    "            ) \n",
    "            self.trade_just_closed = False\n",
    "        \n",
    "        else:\n",
    "            self.realized_pnl = np.zeros(size)\n",
    "            \n",
    "        #print('self.realized_pnl:', self.realized_pnl)  \n",
    "        return self.realized_pnl\n",
    "    \n",
    "    def get_norm_broker_value(self, size=1):\n",
    "        \"\"\"\n",
    "        Broker value, ajusted wrt to max loss/target values, normalized in [-1,1].\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\"\n",
    "        self.norm_broker_value = self.norm_value(\n",
    "            self.stats.broker.value.get(size=size),\n",
    "            self.env.broker.startingcash,\n",
    "            self.p.drawdown_call,\n",
    "            self.p.target_call,\n",
    "        )\n",
    "        return self.norm_broker_value \n",
    "    \n",
    "    def get_reward(self):\n",
    "        \"\"\"\n",
    "        Defines reward as weighted sum of portfolio performance statisitics averaged over last frame-skip period.\n",
    "        \"\"\"\n",
    "        # All reward terms for this step are already computed by get_reward(), wich has been called just before.\n",
    "        \n",
    "        # Reward term 1: averaged profit/loss for current opened trade (unrealized p/l):\n",
    "        avg_unrealised_pnl = np.average(self.unrealized_pnl)\n",
    " \n",
    "        # Reward term 2: averaged broker value, normalized wrt to max drawdown and target bounds.\n",
    "        avg_norm_broker_value = np.average(self.norm_broker_value )\n",
    "        \n",
    "        # Reward term 3: normalized single trade realized profit/loss:\n",
    "        avg_realized_pnl = np.average(self.realized_pnl)\n",
    "            \n",
    "        # Weights are subject to tune, \n",
    "        # absolute reward walue should be consistent with a3c rollout length\n",
    "        # to keep gradients sane (~/2) but not slow down training:\n",
    "        self.reward =  (avg_unrealised_pnl + 0.01 * avg_norm_broker_value + 10 * avg_realized_pnl) / 2\n",
    "        \n",
    "        return self.reward \n",
    "\n",
    "class RewardObserver(bt.observer.Observer):\n",
    "    \"\"\" \n",
    "    Adds reward visualisation to episode plot.\n",
    "    \"\"\"\n",
    "    lines = ('reward',)\n",
    "    plotinfo = dict(plot=True, subplot=True)\n",
    "    \n",
    "    plotlines = dict(\n",
    "        reward=dict(markersize=4.0, color='green', fillstyle='full'),\n",
    "    )\n",
    "    \n",
    "    def next(self):\n",
    "        self.lines.reward[0] = self._owner.reward\n",
    "\n",
    "########################################################       \n",
    "\n",
    "\n",
    "# Set backtesting engine parameters:\n",
    "\n",
    "time_embed_dim = 16\n",
    "\n",
    "state_shape = {\n",
    "    'raw_state': spaces.Box(low=-100, high=100, shape=(time_embed_dim, 4)),\n",
    "    'model_input': spaces.Box(low=-100, high=100, shape=(time_embed_dim, 11)),\n",
    "}\n",
    "\n",
    "MyCerebro = bt.Cerebro()\n",
    "\n",
    "MyCerebro.addstrategy(\n",
    "    MyStrategy,\n",
    "    state_shape=state_shape,\n",
    "    portfolio_actions=('hold', 'buy', 'sell', 'close'),\n",
    "    drawdown_call=5, # max % to loose, in percent of initial cash\n",
    "    target_call=8,  # max % to win, same\n",
    "    skip_frame=10,\n",
    ")\n",
    "\n",
    "# Set leveraged account:\n",
    "MyCerebro.broker.setcash(2000)\n",
    "MyCerebro.broker.setcommission(commission=0.0001, leverage=10.0) # commisssion to imitate spread\n",
    "MyCerebro.addsizer(bt.sizers.SizerFix, stake=5000,)  \n",
    "\n",
    "MyCerebro.addanalyzer(bt.analyzers.DrawDown)\n",
    "\n",
    "MyCerebro.addobserver(RewardObserver)\n",
    "\n",
    "MyDataset = BTgymDataset(\n",
    "    #filename='../data/DAT_ASCII_EURUSD_M1_2016.csv',\n",
    "    filename='../data/DAT_ASCII_EURUSD_M1_201703.csv',\n",
    "    #filename='../data/test_sine_1min_period256_delta0002.csv',\n",
    "    start_weekdays=[0, 1, 2, 3, 4],\n",
    "    episode_len_days=1,\n",
    "    episode_len_hours=2,\n",
    "    episode_len_minutes=0,\n",
    "    start_00=False,\n",
    "    time_gap_hours=6,\n",
    ")\n",
    "env_config = dict(\n",
    "    dataset=MyDataset,\n",
    "    engine=MyCerebro,\n",
    "    render_modes=['episode', 'human', 'model_input'],\n",
    "    render_state_as_image=True,\n",
    "    render_ylabel='SMA_log_gradients and broker stat.',\n",
    "    render_size_episode=(12,8),\n",
    "    render_size_human=(10, 5),\n",
    "    render_size_state=(10, 5),\n",
    "    render_dpi=75,\n",
    "    port=5000,\n",
    "    data_port=4999,\n",
    "    connect_timeout=60,\n",
    "    verbose=0,\n",
    ")\n",
    "# Set tensorflow distributed cluster and a3c configuration:\n",
    "cluster_config = dict(\n",
    "    host='127.0.0.1',\n",
    "    port=12222,\n",
    "    num_workers=8,  # ~ num of CPU cores\n",
    "    num_ps=1,\n",
    "    log_dir='./tmp/a3c_test_4',\n",
    ")\n",
    "launcher = Launcher(\n",
    "    cluster_config=cluster_config,\n",
    "    env_class=BTgymEnv,\n",
    "    env_config=env_config,\n",
    "    policy_class=LSTMPolicy1D,\n",
    "    policy_config={'lstm_layers': (256,)}, # TODO: multi-layer, phased/grid LSTM\n",
    "    rollout_length=20,\n",
    "    model_beta=0.02,  # entropy regularization, TODO: make it log-unform over ~[0.1, 0.01]\n",
    "    opt_learn_rate=1e-4,  # adam learn rate,  TODO: RMSProp with shared stats, log-uniform learn rate\n",
    "    test_mode=False,\n",
    "    train_steps=1000000000,\n",
    "    model_summary_freq=20,\n",
    "    episode_summary_freq=1,\n",
    "    env_render_freq=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "launcher.run()\n",
    "\n",
    "# To track performance: [shell]:  tensorboard --logdir './tmp/a3c_test_3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just in case: for manual environment testing :\n",
    "\n",
    "env_config.update({'port': 5090, 'data_port': 5089})\n",
    "env = BTgymEnv(**env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
