{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  test 4:\n",
    "-----\n",
    "\n",
    "##### data:\n",
    "real data, small set\n",
    "\n",
    "##### topics:\n",
    "- observation state shaping\n",
    "- reward shaping\n",
    "- policy architecture\n",
    "\n",
    "##### abstract:\n",
    "A3C tweaked to ~~solve~~ overfit on dataset of 1 month of EURUSD 1-minute bar data in ~ 10M iterations\n",
    "using hand-coded state features and simple policy estimator without convolution layers.\n",
    "\n",
    "##### key points:\n",
    "- using broker/portfolio statistics to augument state observation ('inner' state);\n",
    "- hard-coded (e.g. not learnable, data-independend) \"outer\" observation state features as bank of open-price \n",
    "  simple moving averages gradients along features axis;\n",
    "- feeding concatenated 'inner' and 'outer' part of state directly to LSTM layer;\n",
    "- shaping reward as weighted sum of averaged broker statisitcs:\n",
    "        - unrealised profit/loss;\n",
    "        - realized trade result;\n",
    "        - current portfolio value;\n",
    "- allowing variable trade position size;\n",
    "- using env.skip_frame=10. Introducing prior actions distribution seems critical to agent performance. I wasn't been able to acheve good results with skip_frame < 8. Not a proper way to shape priors, but working for now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import backtrader.indicators as btind\n",
    "import numpy as np\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "from btgym import BTgymEnv, BTgymStrategy, BTgymDataset\n",
    "\n",
    "from btgym.a3c import Launcher, BaseLSTMPolicy\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "from tensorflow.contrib.layers import flatten as batch_flatten\n",
    "from tensorflow.python.util.nest import flatten as flatten_nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DirectLSTMPolicy(BaseLSTMPolicy):\n",
    "    \"\"\"\n",
    "    Simplest A3C LSTM policy. Directly feeds `hard coded` state features into LSTM layer.\n",
    "    \"\"\"\n",
    "    def __init__(self, ob_space, ac_space, num_filters=32, filter_size=3, stride=2,\n",
    "                 lstm_class=rnn.BasicLSTMCell, lstm_layers=(256,)):\n",
    "        \n",
    "        self.x = x = tf.placeholder(tf.float32, [None] + list(ob_space), name='x_in_pl')\n",
    "\n",
    "        # Run LSTM along rollout time dimension etc:\n",
    "        super(DirectLSTMPolicy, self).__init__(x, ob_space, ac_space, lstm_class, lstm_layers)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyStrategy(BTgymStrategy):\n",
    "    \"\"\"\n",
    "    BT server inner computation startegy tuned to pass small real data test.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyStrategy, self).__init__(**kwargs)\n",
    "        \n",
    "        self.dim_time = self.p.state_shape['raw_state'].shape[0] \n",
    "\n",
    "        self.trade_just_closed = False\n",
    "        self.trade_result = None\n",
    "        \n",
    "        self.unrealized_pnl = None\n",
    "        self.norm_broker_value = None\n",
    "        self.avg_norm_broker_cash = 0 # not used\n",
    "        self.realized_pnl = None\n",
    "\n",
    "        self.realized_broker_value = self.env.broker.startingcash\n",
    "        self.episode_result = 0  # not used\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.avg_period = 10 # not used\n",
    "        \n",
    "        # Z-normalization over entire dataset of O-prices: NOT used.\n",
    "        #mean = np.average(self.p.dataset_stat[1:3].values[0,:-1])\n",
    "        #std = np.average(self.p.dataset_stat[1:3].values[1,:-1])\n",
    "        #self.x = (self.datas[0] - mean) / std \n",
    "        \n",
    "        self.x = self.datas[0] # using O-price as signal. \n",
    "        \n",
    "        # Signal features:\n",
    "        self.data.sma_4 = btind.SimpleMovingAverage(self.x, period=4)\n",
    "        self.data.sma_8 = btind.SimpleMovingAverage(self.x, period=8)\n",
    "        self.data.sma_16 = btind.SimpleMovingAverage(self.x, period=16)\n",
    "        self.data.sma_32 = btind.SimpleMovingAverage(self.x, period=32)\n",
    "        self.data.sma_64 = btind.SimpleMovingAverage(self.x, period=64)\n",
    "        #self.data.sma_128 = btind.SimpleMovingAverage(self.x, period=128)\n",
    "        #self.data.sma_256 = btind.SimpleMovingAverage(self.x, period=256) \n",
    "\n",
    "        # Service sma to get correct first features values:\n",
    "        self.data.dim_sma = btind.SimpleMovingAverage(\n",
    "            self.datas[0],\n",
    "            period=(64 + self.dim_time)\n",
    "        )\n",
    "        self.data.dim_sma.plotinfo.plot = False\n",
    "        \n",
    "    def notify_trade(self, trade):    \n",
    "        if trade.isclosed:\n",
    "            # Set trade flag [True if trade have been closed within last frame-skip period] \n",
    "            # and store trade result:\n",
    "            self.trade_just_closed = True\n",
    "            self.trade_result = trade.pnlcomm\n",
    "            \n",
    "            # Store realized prtfolio value:\n",
    "            self.realized_broker_value = self.broker.get_value()\n",
    " \n",
    "    def get_state(self):\n",
    "        \"\"\" \n",
    "        Returns obs. state as matrix of signal features gradients\n",
    "        and portfolio statistics.\n",
    "        \"\"\"\n",
    "        T = 1e4 # EURUSD\n",
    "        #T = 1e2 # EURUSD, Z-norm\n",
    "        #T = 1 # BTCUSD\n",
    "      \n",
    "        x = np.stack(\n",
    "            [\n",
    "                np.frombuffer(self.x.get(size=self.dim_time)),\n",
    "                np.frombuffer(self.data.sma_4.get(size=self.dim_time)), \n",
    "                np.frombuffer(self.data.sma_8.get(size=self.dim_time)), \n",
    "                np.frombuffer(self.data.sma_16.get(size=self.dim_time)), \n",
    "                np.frombuffer(self.data.sma_32.get(size=self.dim_time)),\n",
    "                np.frombuffer(self.data.sma_64.get(size=self.dim_time)),\n",
    "                #np.frombuffer(self.data.sma_128.get(size=self.dim_time)),\n",
    "                #np.frombuffer(self.data.sma_256.get(size=self.dim_time)), \n",
    "            ], \n",
    "            axis=-1\n",
    "        )\n",
    "            \n",
    "        # Amplified gradient along features axis:\n",
    "        x = np.gradient(x, axis=1) * T\n",
    "        \n",
    "        # Log-scale: NOT used. Seems to hurt performance.\n",
    "        #x = self.log_transform(x)\n",
    "        \n",
    "        # Add inner state statistic:\n",
    "        _ = self.get_unrealized_pnl(size=self.dim_time)\n",
    "        _ = self.get_realized_pnl(size=self.dim_time, gamma=1.0)\n",
    "        _ = self.get_norm_broker_value(size=self.dim_time)\n",
    "        _ = self.get_norm_broker_cash(size=self.dim_time)\n",
    "        _ = self.get_norm_position_size(size=self.dim_time)\n",
    "        \n",
    "        x = np.concatenate(\n",
    "            [\n",
    "                x,\n",
    "                self.unrealized_pnl[..., None],\n",
    "                self.realized_pnl[..., None],\n",
    "                #self.norm_broker_value[..., None],  # seems to be indifferent\n",
    "                self.norm_broker_cash[..., None],\n",
    "                self.norm_position_size[..., None],\n",
    "            ],\n",
    "            axis=-1\n",
    "        )\n",
    "        \n",
    "        self.state['raw_state'] = self.raw_state\n",
    "                \n",
    "        self.state['model_input'] = x\n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "    def log_transform(self, x):\n",
    "        return np.sign(x) * np.log(np.fabs(x) + 1)\n",
    "\n",
    "    def norm_log_value(self, current_value, start_value, drawdown_call, target_call, epsilon=1e-4):\n",
    "        \"\"\"Current value log-normalized in [-1,1] wrt p/l limits.\"\"\"\n",
    "        x = np.asarray(current_value)\n",
    "        x = (x / start_value - 1) * 100\n",
    "        x = (x - target_call)/(drawdown_call+target_call) + 1\n",
    "        x = np.clip(x, epsilon, 1 - epsilon)\n",
    "        x = 1 - 2 * np.log(x) / np.log( epsilon)\n",
    "        return x\n",
    "    \n",
    "    def norm_value(self, current_value, start_value, drawdown_call, target_call, epsilon=1e-8):\n",
    "        \"\"\"Current value normalized in [-1,1] wrt upper and lower bounds\"\"\"\n",
    "        x = np.asarray(current_value)\n",
    "        x = (x / start_value - 1) * 100\n",
    "        x = (x - target_call)/(drawdown_call+target_call) + 1\n",
    "        x = 2 * np.clip(x, epsilon, 1 - epsilon) - 1 \n",
    "        return x\n",
    "    \n",
    "    def decayed_result(self, trade_result, current_value, start_value, drawdown_call, target_call, gamma=1.0):\n",
    "        \"Normalized in [-1,1] trade result, lineary decayed wrt current value.\"\n",
    "        target_value = start_value * (1 + target_call/100)\n",
    "        value_range = start_value * (drawdown_call + target_call)/100\n",
    "        decay = (gamma - 1) * (current_value - target_value) / value_range + gamma\n",
    "        x = trade_result * decay / value_range\n",
    "        return x\n",
    "    \n",
    "    def get_unrealized_pnl(self, size=1):\n",
    "        \"\"\"\n",
    "        Normalized profit/loss for current opened trade (unrealized p/l).\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\"\n",
    "        x = np.asarray(self.stats.broker.value.get(size=size)) - self.realized_broker_value\n",
    "        self.unrealized_pnl = x / self.env.broker.startingcash /(self.p.drawdown_call + self.p.target_call) * 100\n",
    "        return self.unrealized_pnl\n",
    "    \n",
    "    def get_realized_pnl(self, size=1, gamma=1.0):\n",
    "        \"\"\"\n",
    "        Returns single trade realized profit/loss, normalized and gamma-adjusted to current broker value.\n",
    "        Returns vector of filled with p/l value of length `size`.\n",
    "        \"\"\"\n",
    "        if self.trade_just_closed:\n",
    "            self.realized_pnl = self.decayed_result(\n",
    "                self.trade_result,\n",
    "                np.asarray(self.stats.broker.value.get(size=size)),\n",
    "                self.env.broker.startingcash,\n",
    "                self.p.drawdown_call,\n",
    "                self.p.target_call,\n",
    "                gamma=gamma\n",
    "            ) \n",
    "            self.trade_just_closed = False\n",
    "        \n",
    "        else:\n",
    "            self.realized_pnl = np.zeros(size)\n",
    "            \n",
    "        #print('self.realized_pnl:', self.realized_pnl)  \n",
    "        return self.realized_pnl\n",
    "    \n",
    "    def get_norm_broker_value(self, size=1):\n",
    "        \"\"\"\n",
    "        Broker value, normalized in [-1,1] wrt profit/loss limits.\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\"\n",
    "        self.norm_broker_value = self.norm_value(\n",
    "            self.stats.broker.value.get(size=size),\n",
    "            self.env.broker.startingcash,\n",
    "            self.p.drawdown_call,\n",
    "            self.p.target_call,\n",
    "        )\n",
    "        return self.norm_broker_value \n",
    "    \n",
    "    def get_norm_broker_cash(self, size=1):\n",
    "        \"\"\"\n",
    "        Normalized broker cash.\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\" \n",
    "        self.norm_broker_cash = self.norm_value(\n",
    "            self.stats.broker.cash.get(size=size),\n",
    "            self.env.broker.startingcash,\n",
    "            99.0,\n",
    "            self.p.target_call,\n",
    "        )\n",
    "        return self.norm_broker_cash \n",
    "    \n",
    "    def get_norm_position_size(self, size=1):\n",
    "        \"\"\"\n",
    "        Normalized in (-1, 1) position size.\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\"\n",
    "        self.norm_position_size = np.asarray(\n",
    "            self.stats.position.value.get(size=size)\n",
    "        ) / (self.env.broker.startingcash * (np.asarray(self.stats.position.leverage.get(size=size)) + 1e-2))\n",
    "        return self.norm_position_size\n",
    "    \n",
    "    def get_reward(self):\n",
    "        \"\"\"\n",
    "        Defines reward as weighted sum of portfolio performance statisitics\n",
    "        averaged over time-embedding period.\n",
    "        \"\"\"\n",
    "        # All reward terms for this step are already computed by get_state(), wich has been called just before.\n",
    "        \n",
    "        # Reward term 1: averaged profit/loss for current opened trade (unrealized p/l):\n",
    "        avg_unrealised_pnl = np.average(self.unrealized_pnl)\n",
    " \n",
    "        # Reward term 2: averaged broker value, normalized wrt to max drawdown and target bounds.\n",
    "        avg_norm_broker_value = np.average(self.norm_broker_value )\n",
    "        \n",
    "        # Reward term 3: normalized single trade realized profit/loss:\n",
    "        avg_realized_pnl = np.average(self.realized_pnl)\n",
    "            \n",
    "        # Weights are subject to tune, \n",
    "        # absolute reward walue should be consistent with a3c rollout length\n",
    "        # to keep gradients sane (~/2) while not slowing down training:\n",
    "        self.reward =  (avg_unrealised_pnl + 0.01 * avg_norm_broker_value + 10 * avg_realized_pnl) / 2\n",
    "        \n",
    "        return self.reward \n",
    "\n",
    "class Position(bt.observer.Observer):\n",
    "    \"\"\" \n",
    "    Keeps track of position size and effective leverage.\n",
    "    \"\"\"\n",
    "    lines = ('value', 'leverage')\n",
    "    plotinfo = dict(plot=True, subplot=True)\n",
    "    plotlines = dict(\n",
    "        value=dict(marker='.', markersize=1.0, color='blue', fillstyle='full'),\n",
    "        leverage=dict(_plotskip='True',),\n",
    "    )\n",
    "    \n",
    "    def next(self):\n",
    "        self.lines.value[0] = self._owner.position.size\n",
    "        self.lines.leverage[0] = self._owner.env.broker.get_leverage()\n",
    "    \n",
    "    \n",
    "class Reward(bt.observer.Observer):\n",
    "    \"\"\" \n",
    "    Keeps track of reward values.\n",
    "    \"\"\"\n",
    "    lines = ('value',)\n",
    "    plotinfo = dict(plot=True, subplot=True)\n",
    "    \n",
    "    plotlines = dict(\n",
    "        value=dict(markersize=4.0, color='green', fillstyle='full'),\n",
    "    )\n",
    "    \n",
    "    def next(self):\n",
    "        self.lines.value[0] = self._owner.reward\n",
    "\n",
    "########################################################       \n",
    "\n",
    "\n",
    "# Set backtesting engine parameters:\n",
    "\n",
    "time_embed_dim = 30\n",
    "\n",
    "state_shape = {\n",
    "    'raw_state': spaces.Box(low=-100, high=100, shape=(time_embed_dim, 4)),\n",
    "    'model_input': spaces.Box(low=-100, high=100, shape=(time_embed_dim, 10)),\n",
    "}\n",
    "\n",
    "MyCerebro = bt.Cerebro()\n",
    "\n",
    "MyCerebro.addstrategy(\n",
    "    MyStrategy,\n",
    "    state_shape=state_shape,\n",
    "    portfolio_actions=('hold', 'buy', 'sell', 'close'),\n",
    "    drawdown_call=5, # max % to loose, in percent of initial cash\n",
    "    target_call=15,  # max % to win, same\n",
    "    skip_frame=10,\n",
    ")\n",
    "\n",
    "# Set leveraged account:\n",
    "MyCerebro.broker.setcash(2000)\n",
    "MyCerebro.broker.setcommission(commission=0.0001, leverage=10.0) # commisssion to imitate spread\n",
    "MyCerebro.addsizer(bt.sizers.SizerFix, stake=5000,)  \n",
    "\n",
    "MyCerebro.addanalyzer(bt.analyzers.DrawDown)\n",
    "MyCerebro.addobserver(Reward)\n",
    "MyCerebro.addobserver(Position)\n",
    "\n",
    "MyDataset = BTgymDataset(\n",
    "    #filename='../data/DAT_ASCII_EURUSD_M1_2016.csv',\n",
    "    filename='../data/DAT_ASCII_EURUSD_M1_201703.csv',\n",
    "    #filename='../data/test_sine_1min_period256_delta0002.csv',\n",
    "    start_weekdays=[0, 1, 2, 3, 4],\n",
    "    episode_len_days=0,\n",
    "    episode_len_hours=23,\n",
    "    episode_len_minutes=55,\n",
    "    start_00=False,\n",
    "    time_gap_hours=6,\n",
    ")\n",
    "env_config = dict(\n",
    "    dataset=MyDataset,\n",
    "    engine=MyCerebro,\n",
    "    render_modes=['episode', 'human', 'model_input'],\n",
    "    render_state_as_image=True,\n",
    "    render_ylabel='SMA_bank_gradients and broker stat.',\n",
    "    render_size_episode=(12,8),\n",
    "    render_size_human=(10, 5),\n",
    "    render_size_state=(10, 5),\n",
    "    render_dpi=75,\n",
    "    port=5000,\n",
    "    data_port=4999,\n",
    "    connect_timeout=60,\n",
    "    verbose=0,\n",
    ")\n",
    "# Set tensorflow distributed cluster and a3c configuration:\n",
    "cluster_config = dict(\n",
    "    host='127.0.0.1',\n",
    "    port=12222,\n",
    "    num_workers=8,  # ~ num of CPU cores\n",
    "    num_ps=1,\n",
    "    log_dir='./tmp/a3c_test_4_0',\n",
    ")\n",
    "launcher = Launcher(\n",
    "    cluster_config=cluster_config,\n",
    "    env_class=BTgymEnv,\n",
    "    env_config=env_config,\n",
    "    policy_class=DirectLSTMPolicy,\n",
    "    policy_config={'lstm_layers': (256,)}, # TODO: multi-layer, phased/grid LSTM\n",
    "    rollout_length=20,\n",
    "    model_beta=0.01,  # entropy regularization, TODO: make it log-unform over ~[0.1, 0.01]\n",
    "    opt_learn_rate=1e-4,  # adam learn rate,  TODO: RMSProp with shared stats, log-uniform learn rate + annealing\n",
    "    test_mode=False,\n",
    "    train_steps=1000000000,\n",
    "    model_summary_freq=20,\n",
    "    episode_summary_freq=1,\n",
    "    env_render_freq=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "launcher.run()\n",
    "\n",
    "# To track performance: [shell]:  tensorboard --logdir './tmp/a3c_test_4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just in case: \n",
    "\n",
    "#for manual environment testing :\n",
    "env_config.update({'port': 5090, 'data_port': 5089})\n",
    "env = BTgymEnv(**env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "o = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "step = env.step(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# policy graph test:\n",
    "tf.reset_default_graph()\n",
    "pi = LSTMPolicy1D((10,2),(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
