{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimized sine-wave reject test.\n",
    "\n",
    "This notebook presents some basic ideas on state presentation, reward shaping, model architecture and hyperparameters choice. With those tweaks sine-wave sanity test is converging faster and with greater stability.\n",
    "\n",
    "\n",
    "Key points:\n",
    "- using 1d convolutions along time-embedding dimension in policy model;\n",
    "- shaping observation state as bank of open-price simple moving averages with increasing time-periods;\n",
    "- ensuring signal stationarity by taking gradients along features axis;\n",
    "- fighting signal outliers with log-scaling;\n",
    "- making reward a function of current unrealised profit/loss with addition of  one-time amplified realized          trade result;\n",
    "- allowing variable trade position size.\n",
    "- using env.skip_frame=10. Introducing prior actions distribution seems critical to agent performance. I            wasn't been able to acheve good results with skip_frame < 8. Obviously it's not optimal way to shape              priors, but working for now;\n",
    "- lowing entropy regularization term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import backtrader.indicators as btind\n",
    "import numpy as np\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "from btgym import BTgymEnv, BTgymStrategy, BTgymDataset\n",
    "\n",
    "from btgym.a3c import Launcher, BaseLSTMPolicy\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "from tensorflow.python.util.nest import flatten as flatten_nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMPolicy1D(BaseLSTMPolicy):\n",
    "    \"\"\"\n",
    "    A3C LSTM policy model with 1D convolution layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, ob_space, ac_space, num_filters=32, filter_size=5, stride=2,\n",
    "                 lstm_class=rnn.BasicLSTMCell, lstm_layers=(256,)):\n",
    "        \n",
    "        self.x = x = tf.placeholder(tf.float32, [None] + list(ob_space), name='x_in_pl')\n",
    "\n",
    "        # Conv layers:\n",
    "        for i in range(4):\n",
    "            x = tf.nn.elu(self.conv1d(x, num_filters, \"l{}\".format(i + 1), filter_size, stride))\n",
    "      \n",
    "        # Run LSTM along rollout time dimension and evrything else:\n",
    "        super(LSTMPolicy1D, self).__init__(x, ob_space, ac_space, lstm_class, lstm_layers)\n",
    "           \n",
    "    def conv1d(self, x, num_filters, name, filter_size=3, stride=2, pad=\"SAME\", dtype=tf.float32,\n",
    "               collections=None):\n",
    "        with tf.variable_scope(name):\n",
    "            stride_shape =  stride\n",
    "            \n",
    "            #print('stride_shape:',stride_shape)\n",
    "            \n",
    "            filter_shape = [filter_size, int(x.get_shape()[-1]), num_filters]\n",
    "            \n",
    "            #print('filter_shape:', filter_shape)\n",
    "            \n",
    "            # there are \"num input feature maps * filter height * filter width\"\n",
    "            # inputs to each hidden unit\n",
    "            fan_in = np.prod(filter_shape[:2])\n",
    "            \n",
    "            # each unit in the lower layer receives a gradient from:\n",
    "            # \"num output feature maps * filter height * filter width\" /\n",
    "            #   pooling size\n",
    "            fan_out = np.prod(filter_shape[:1]) * num_filters\n",
    "\n",
    "            # initialize weights with random weights\n",
    "            w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "\n",
    "            w = tf.get_variable(\"W\", filter_shape, dtype, tf.random_uniform_initializer(-w_bound, w_bound),\n",
    "                                collections=collections)\n",
    "            b = tf.get_variable(\"b\", [1, 1, num_filters], initializer=tf.constant_initializer(0.0),\n",
    "                                collections=collections)\n",
    "            return tf.nn.conv1d(x, w, stride_shape, pad) + b\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class MyStrategy(BTgymStrategy):\n",
    "    \"\"\"\n",
    "    BT server inner computation startegy tuned to pass sine-wave sanity test.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyStrategy, self).__init__(**kwargs)\n",
    "        \n",
    "        self.dim_time = self.p.state_shape['raw_state'].shape[0] \n",
    "\n",
    "        self.trade_just_closed = False\n",
    "        self.trade_result = None\n",
    "\n",
    "        self.realised_broker_value = self.env.broker.startingcash\n",
    "        self.episode_result = 0\n",
    "        self.reward = 0\n",
    "        \n",
    "        self.avg_period = 10 # should be somehow consistent with skip_frame value\n",
    "        \n",
    "        # Signal features:\n",
    "        self.data.sma_4 = btind.SimpleMovingAverage(self.datas[0], period=4)\n",
    "        self.data.sma_8 = btind.SimpleMovingAverage(self.datas[0], period=8)\n",
    "        self.data.sma_16 = btind.SimpleMovingAverage(self.datas[0], period=16)\n",
    "        self.data.sma_32 = btind.SimpleMovingAverage(self.datas[0], period=32)\n",
    "        self.data.sma_64 = btind.SimpleMovingAverage(self.datas[0], period=64)\n",
    "        self.data.sma_128 = btind.SimpleMovingAverage(self.datas[0], period=128)\n",
    "        self.data.sma_256 = btind.SimpleMovingAverage(self.datas[0], period=256)\n",
    "\n",
    "        # Service sma to get correct first features values:\n",
    "        self.data.dim_sma = btind.SimpleMovingAverage(\n",
    "            self.datas[0],\n",
    "            period=(256 + self.dim_time)\n",
    "        )\n",
    "        self.data.dim_sma.plotinfo.plot = False\n",
    "        \n",
    "    def notify_trade(self, trade):    \n",
    "        if trade.isclosed:\n",
    "            # Set trade flag and store trade result:\n",
    "            self.trade_just_closed = True\n",
    "            self.trade_result = trade.pnlcomm\n",
    "            \n",
    "            # Store realized prtfolio value:\n",
    "            self.realised_broker_value = self.broker.get_value()\n",
    " \n",
    "    def get_state(self):\n",
    "        \"\"\" \n",
    "        Computes obs. state as [time_dim, 8] matrix of log-scaled features gradients.\n",
    "        \"\"\"\n",
    "        T = 1e4 # EURUSD\n",
    "        #T = 1 # BTCUSD\n",
    "      \n",
    "        x = np.stack(\n",
    "            [\n",
    "                np.frombuffer(self.data.open.get(size=self.dim_time)),\n",
    "                np.frombuffer(self.data.sma_4.get(size=self.dim_time)), \n",
    "                np.frombuffer(self.data.sma_8.get(size=self.dim_time)), \n",
    "                np.frombuffer(self.data.sma_16.get(size=self.dim_time)), \n",
    "                np.frombuffer(self.data.sma_32.get(size=self.dim_time)),\n",
    "                np.frombuffer(self.data.sma_64.get(size=self.dim_time)),\n",
    "                np.frombuffer(self.data.sma_128.get(size=self.dim_time)),\n",
    "                np.frombuffer(self.data.sma_256.get(size=self.dim_time)), \n",
    "            ], \n",
    "            axis=-1\n",
    "        )\n",
    "        # Amplified gradient along features axis:\n",
    "        x = np.gradient(x, axis=1) * T\n",
    "        \n",
    "        # Log-scale:\n",
    "        x = self.log_transform(x)\n",
    "        \n",
    "        self.state['raw_state'] = self.raw_state\n",
    "                \n",
    "        self.state['model_input'] = x\n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "    def log_transform(self, x):\n",
    "        return np.sign(x) * np.log(np.fabs(x) + 1)\n",
    "\n",
    "    def norm_log_value(self, current_value, start_value, drawdown_call, target_call, epsilon=1e-4):\n",
    "        \"\"\"Current value log-normalized in [-1,1] wrt upper and lower bounds\"\"\"\n",
    "        x = np.asarray(current_value)\n",
    "        x = (x / start_value - 1) * 100\n",
    "        x = (x - target_call)/(drawdown_call+target_call) + 1\n",
    "        x = np.clip(x, epsilon, 1 - epsilon)\n",
    "        x = 1 - 2 * np.log(x) / np.log( epsilon)\n",
    "        return x\n",
    "    \n",
    "    def norm_value(self, current_value, start_value, drawdown_call, target_call, epsilon=1e-8):\n",
    "        \"\"\"Current value normalized in [-1,1] wrt upper and lower bounds\"\"\"\n",
    "        x = np.asarray(current_value)\n",
    "        x = (x / start_value - 1) * 100\n",
    "        x = (x - target_call)/(drawdown_call+target_call) + 1\n",
    "        x = 2 * np.clip(x, epsilon, 1 - epsilon) - 1 \n",
    "        return x\n",
    "    \n",
    "    def decayed_result(self, trade_result, current_value, start_value, drawdown_call, target_call, gamma=0.8):\n",
    "        \"Normalized in [-1,1] trade result, lineary decayed wrt current value.\"\n",
    "        target_value = start_value * (1 + target_call/100)\n",
    "        value_range = start_value * (drawdown_call + target_call)/100\n",
    "        decay = (gamma - 1) * (current_value - target_value) / value_range + gamma\n",
    "        x = trade_result * decay / value_range\n",
    "        return x\n",
    "    \n",
    "    def get_reward(self):\n",
    "        \"\"\"\n",
    "        Defines reward as composition of portfolio performance statisitics.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Reward term 1: averaged profit/loss for current opened trade (unrealized p/l):\n",
    "        unrealised_pnl = np.average(self.stats.broker.value.get(size=self.avg_period))\\\n",
    "            - self.realised_broker_value\n",
    "        \n",
    "        #Normalize:\n",
    "        unrealised_pnl /= self.env.broker.startingcash *(self.p.drawdown_call + self.p.target_call) / 100\n",
    "\n",
    "        # Reward term 2: averaged broker value, normalized wrt to max drawdown and target bounds.\n",
    "        avg_norm_broker_value = self.norm_value(\n",
    "            np.average(self.stats.broker.value.get(size=self.avg_period)),\n",
    "            self.env.broker.startingcash,\n",
    "            self.p.drawdown_call,\n",
    "            self.p.target_call,\n",
    "        ) \n",
    "        \n",
    "        # Reward term 3: normalized single trade realized profit/loss:\n",
    "        if self.trade_just_closed:\n",
    "            realized_pnl = self.decayed_result(\n",
    "                self.trade_result,\n",
    "                np.average(self.stats.broker.value.get(size=self.avg_period)),\n",
    "                self.env.broker.startingcash,\n",
    "                self.p.drawdown_call,\n",
    "                self.p.target_call,\n",
    "                gamma=1.0\n",
    "            ) \n",
    "            self.trade_just_closed = False\n",
    "        \n",
    "        else:\n",
    "            realized_pnl = 0\n",
    "            \n",
    "        # Coefficients are tunable:\n",
    "        self.reward = unrealised_pnl + 1e-2 * avg_norm_broker_value + 10 * realized_pnl\n",
    "        \n",
    "        return self.reward \n",
    "\n",
    "class RewardObserver(bt.observer.Observer):\n",
    "    \"\"\" \n",
    "    Adds reward visualisation to episode plot.\n",
    "    \"\"\"\n",
    "    lines = ('reward',)\n",
    "    plotinfo = dict(plot=True, subplot=True)\n",
    "    \n",
    "    plotlines = dict(\n",
    "        reward=dict(markersize=4.0, color='green', fillstyle='full'),\n",
    "    )\n",
    "    \n",
    "    def next(self):\n",
    "        self.lines.reward[0] = self._owner.reward\n",
    "\n",
    "########################################################       \n",
    "\n",
    "\n",
    "# Set backtesting engine parameters:\n",
    "\n",
    "time_embed_dim = 16\n",
    "\n",
    "state_shape = {\n",
    "    'raw_state': spaces.Box(low=-100, high=100, shape=(time_embed_dim, 4)),\n",
    "    'model_input': spaces.Box(low=-100, high=100, shape=(time_embed_dim, 8)),\n",
    "}\n",
    "\n",
    "MyCerebro = bt.Cerebro()\n",
    "\n",
    "MyCerebro.addstrategy(\n",
    "    MyStrategy,\n",
    "    state_shape=state_shape,\n",
    "    portfolio_actions=('hold', 'buy', 'sell', 'close'),\n",
    "    drawdown_call=5, # max % to loose, in percent of initial cash\n",
    "    target_call=8,  # max % to win, same\n",
    "    skip_frame=10,\n",
    ")\n",
    "\n",
    "# Set leveraged account:\n",
    "MyCerebro.broker.setcash(2000)\n",
    "MyCerebro.broker.setcommission(commission=0.0001, leverage=10.0) # commisssion to imitate spread\n",
    "MyCerebro.addsizer(bt.sizers.SizerFix, stake=5000,)  \n",
    "\n",
    "MyCerebro.addanalyzer(bt.analyzers.DrawDown)\n",
    "\n",
    "MyCerebro.addobserver(RewardObserver)\n",
    "# Dataset:\n",
    "MyDataset = BTgymDataset(\n",
    "    filename='../data/test_sine_1min_period256_delta0002.csv',\n",
    "    start_weekdays=[0, 1, 2, 3, 4],\n",
    "    episode_len_days=1,\n",
    "    episode_len_hours=2,\n",
    "    episode_len_minutes=0,\n",
    "    start_00=False,\n",
    "    time_gap_hours=6,\n",
    ")\n",
    "# Environment parameters:\n",
    "env_config = dict(\n",
    "    dataset=MyDataset,\n",
    "    engine=MyCerebro,\n",
    "    render_modes=['episode', 'human', 'model_input'],\n",
    "    render_state_as_image=True,\n",
    "    render_ylabel='SMA_log_gradients',\n",
    "    render_size_episode=(12,8),\n",
    "    render_size_human=(10, 5),\n",
    "    render_size_state=(10, 5),\n",
    "    render_dpi=75,\n",
    "    port=5000,\n",
    "    data_port=4999,\n",
    "    connect_timeout=60,\n",
    "    verbose=0,\n",
    ")\n",
    "# Set tensorflow distributed cluster and a3c configuration:\n",
    "cluster_config = dict(\n",
    "    host='127.0.0.1',\n",
    "    port=12222,\n",
    "    num_workers=8,  # ~ num of CPU cores\n",
    "    num_ps=1,\n",
    "    log_dir='./tmp/a3c_test_4',\n",
    ")\n",
    "launcher = Launcher(\n",
    "    cluster_config=cluster_config,\n",
    "    env_class=BTgymEnv,\n",
    "    env_config=env_config,\n",
    "    policy_class=LSTMPolicy1D,\n",
    "    policy_config={'lstm_layers': (256,)},\n",
    "    rollout_length=20,\n",
    "    model_beta=0.02,  # entropy regularization, shouldbe in ~[0.1, 0.01]\n",
    "    opt_learn_rate=1e-4,  # adam learn rate\n",
    "    test_mode=False,\n",
    "    train_steps=1000000000,\n",
    "    model_summary_freq=20,\n",
    "    episode_summary_freq=1,\n",
    "    env_render_freq=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train it:\n",
    "launcher.run()\n",
    "\n",
    "# To track performance: [shell]:  tensorboard --logdir './tmp/a3c_test_4'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Just in case (for manual environment testing):\n",
    "\n",
    "env_config.update({'port': 5090, 'data_port': 5089})\n",
    "env = BTgymEnv(**env_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
