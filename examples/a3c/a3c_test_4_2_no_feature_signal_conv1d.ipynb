{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  test 4.2:\n",
    "-----\n",
    "\n",
    "##### data:\n",
    "real data, small set\n",
    "\n",
    "##### topics:\n",
    "- observation state shaping\n",
    "- reward shaping\n",
    "- policy architecture\n",
    "\n",
    "##### abstract:\n",
    "Using convolution layers to to shape state features, added reward term to force position close at the end of episode.\n",
    "\n",
    "##### key points:\n",
    "- using broker/portfolio statistics to augument state observation ('inner' state);\n",
    "- using 1d convolutions along time-embedding dimension for 'outer' state signal;\n",
    "- outer observation state is matrix of shape [dim_time_embedding, 3] of:\n",
    "        - one-step difference of Open price;\n",
    "        - difference in current Open and High price;\n",
    "        - difference in current Open and Low price;\n",
    "- feeding 'inner' part of state directly to LSTM layer;\n",
    "- using kind of \"skip connection\" to feed `outer` state both to convolution layers and to LSTM layer, wich seems to give faster convergence;\n",
    "- shaping reward as weighted sum of averaged broker statisitcs: \n",
    "        - unrealised profit/loss;\n",
    "        - realized trade result;\n",
    "        - current portfolio value;\n",
    "        - [added] prohibitive penalty for opened position at the end of episode;\n",
    "- variable trade position size;\n",
    "- using env.skip_frame=10.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import backtrader as bt\n",
    "import backtrader.indicators as btind\n",
    "import numpy as np\n",
    "\n",
    "from gym import spaces\n",
    "\n",
    "from btgym import BTgymEnv, BTgymStrategy, BTgymDataset\n",
    "\n",
    "from btgym.a3c import Launcher, BaseLSTMPolicy\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.contrib.rnn as rnn\n",
    "from tensorflow.contrib.layers import flatten as batch_flatten\n",
    "from tensorflow.python.util.nest import flatten as flatten_nested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LSTMPolicy1D(BaseLSTMPolicy):\n",
    "    \"\"\"\n",
    "    A3C LSTM policy with 1D convolutions.\n",
    "    \"\"\"\n",
    "    def __init__(self, ob_space, ac_space, num_filters=32, filter_size=3, stride=2,\n",
    "                 lstm_class=rnn.BasicLSTMCell, lstm_layers=(256,)):\n",
    "        \n",
    "        self.x = x = tf.placeholder(tf.float32, [None] + list(ob_space), name='x_in_pl')\n",
    "        \n",
    "        # Outer state:\n",
    "        x1 = x[..., :3]\n",
    "        \n",
    "        # Inner state:\n",
    "        #x2 = x[..., 3:]\n",
    "\n",
    "        # Pass outer state through conv layers:\n",
    "        for i in range(4):\n",
    "            x1 = tf.nn.elu(self.conv1d(x1, num_filters, \"l{}\".format(i + 1), filter_size, stride))\n",
    "                \n",
    "        x = tf.concat(\n",
    "            [batch_flatten(x1), batch_flatten(x),],\n",
    "            axis=-1\n",
    "        )\n",
    "\n",
    "        \n",
    "        # Run LSTM along rollout time dimension and evrything else:\n",
    "        super(LSTMPolicy1D, self).__init__(x, ob_space, ac_space, lstm_class, lstm_layers)\n",
    "           \n",
    "    def conv1d(self, x, num_filters, name, filter_size=3, stride=2, pad=\"SAME\", dtype=tf.float32,\n",
    "               collections=None):\n",
    "        with tf.variable_scope(name):\n",
    "            stride_shape =  stride\n",
    "            \n",
    "            #print('stride_shape:',stride_shape)\n",
    "            \n",
    "            filter_shape = [filter_size, int(x.get_shape()[-1]), num_filters]\n",
    "            \n",
    "            #print('filter_shape:', filter_shape)\n",
    "            \n",
    "            # there are \"num input feature maps * filter height * filter width\"\n",
    "            # inputs to each hidden unit\n",
    "            fan_in = np.prod(filter_shape[:2])\n",
    "            \n",
    "            # each unit in the lower layer receives a gradient from:\n",
    "            # \"num output feature maps * filter height * filter width\" /\n",
    "            #   pooling size\n",
    "            fan_out = np.prod(filter_shape[:1]) * num_filters\n",
    "\n",
    "            # initialize weights with random weights\n",
    "            w_bound = np.sqrt(6. / (fan_in + fan_out))\n",
    "\n",
    "            w = tf.get_variable(\"W\", filter_shape, dtype, tf.random_uniform_initializer(-w_bound, w_bound),\n",
    "                                collections=collections)\n",
    "            b = tf.get_variable(\"b\", [1, 1, num_filters], initializer=tf.constant_initializer(0.0),\n",
    "                                collections=collections)\n",
    "            return tf.nn.conv1d(x, w, stride_shape, pad) + b\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class MyStrategy(BTgymStrategy):\n",
    "    \"\"\"\n",
    "    BT server inner computation startegy tuned to pass small real data test.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **kwargs):\n",
    "        super(MyStrategy, self).__init__(**kwargs)\n",
    "        \n",
    "        self.dim_time = self.p.state_shape['raw_state'].shape[0] \n",
    "\n",
    "        self.trade_just_closed = False\n",
    "        self.trade_result = None\n",
    "        \n",
    "        self.unrealized_pnl = None\n",
    "        self.norm_broker_value = None\n",
    "        self.avg_norm_broker_cash = 0 # not used\n",
    "        self.realized_pnl = None\n",
    "\n",
    "        self.realized_broker_value = self.env.broker.startingcash\n",
    "        self.episode_result = 0  # not used\n",
    "        self.reward = 0\n",
    "\n",
    "        # Signal (`outer` state) presentation datalines:\n",
    "        \n",
    "        # `Open` channel is one step difference of Open price;\n",
    "        # `High` and `Low` channels are differences\n",
    "        # beetwen current Open price and  current High or Low prices respectively:  \n",
    "        self.channel_O = bt.Sum(self.data.open, - self.data.open(-1))\n",
    "        self.channel_H = bt.Sum(self.data.high, - self.data.open)\n",
    "        self.channel_L = bt.Sum(self.data.low,  - self.data.open)\n",
    "\n",
    "        # Service sma to get correct first features values:\n",
    "        self.data.dim_sma = btind.SimpleMovingAverage(\n",
    "            self.datas[0],\n",
    "            period=(self.dim_time)\n",
    "        )\n",
    "        self.data.dim_sma.plotinfo.plot = False\n",
    "        \n",
    "    def notify_trade(self, trade):    \n",
    "        if trade.isclosed:\n",
    "            # Set trade flag [True if trade have been closed within last frame-skip period] \n",
    "            # and store trade result:\n",
    "            self.trade_just_closed = True\n",
    "            self.trade_result = trade.pnlcomm\n",
    "            \n",
    "            # Store realized prtfolio value:\n",
    "            self.realized_broker_value = self.broker.get_value()\n",
    " \n",
    "    def get_state(self):\n",
    "        \"\"\" \n",
    "        Computes obs. state as matrix of log-scaled signal features\n",
    "        and portfolio statistics.\n",
    "        \"\"\"\n",
    "        T = 1e4 # EURUSD\n",
    "        #T = 1e2 # EURUSD, Z-norm\n",
    "        #T = 1 # BTCUSD\n",
    "      \n",
    "        x = np.stack(\n",
    "            [\n",
    "                np.frombuffer(self.channel_O.get(size=self.dim_time)),\n",
    "                np.frombuffer(self.channel_H.get(size=self.dim_time)), \n",
    "                np.frombuffer(self.channel_L.get(size=self.dim_time)), \n",
    "            ], \n",
    "            axis=-1\n",
    "        )\n",
    "            \n",
    "        # Amplified:\n",
    "        x *= T\n",
    "        \n",
    "        # Log-scale: NOT used. Seems to hurt performance.\n",
    "        # x = self.log_transform(x)\n",
    "        \n",
    "        # Add inner state statistic:\n",
    "        _ = self.get_unrealized_pnl(size=self.dim_time)\n",
    "        _ = self.get_realized_pnl(size=self.dim_time, gamma=1.0)\n",
    "        _ = self.get_norm_broker_value(size=self.dim_time)\n",
    "        _ = self.get_norm_broker_cash(size=self.dim_time)\n",
    "        _ = self.get_norm_position_size(size=self.dim_time)\n",
    "        \n",
    "        _ = self.get_norm_position_duration(size=self.dim_time)\n",
    "        _ = self.get_norm_episode_duration(size=self.dim_time)\n",
    "        \n",
    "        \n",
    "        x = np.concatenate(\n",
    "            [\n",
    "                x,\n",
    "                self.unrealized_pnl[..., None],\n",
    "                self.realized_pnl[..., None],\n",
    "                #self.norm_broker_value[..., None],  # seems to be indifferent \n",
    "                self.norm_broker_cash[..., None],\n",
    "                self.norm_position_size[..., None],\n",
    "                self.exp_scale(self.norm_episode_duration, gamma=6)[...,None],\n",
    "                self.exp_scale(self.norm_position_duration, gamma=2)[...,None],\n",
    "            ],\n",
    "            axis=-1\n",
    "        )\n",
    "        \n",
    "        self.state['raw_state'] = self.raw_state\n",
    "                \n",
    "        self.state['model_input'] = x\n",
    "        \n",
    "        return self.state\n",
    "        \n",
    "    def log_transform(self, x):\n",
    "        return np.sign(x) * np.log(np.fabs(x) + 1)\n",
    "\n",
    "    def norm_log_value(self, current_value, start_value, drawdown_call, target_call, epsilon=1e-4):\n",
    "        \"\"\"Current value log-normalized in [-1,1] wrt p/l limits.\"\"\"\n",
    "        x = np.asarray(current_value)\n",
    "        x = (x / start_value - 1) * 100\n",
    "        x = (x - target_call)/(drawdown_call+target_call) + 1\n",
    "        x = np.clip(x, epsilon, 1 - epsilon)\n",
    "        x = 1 - 2 * np.log(x) / np.log( epsilon)\n",
    "        return x\n",
    "    \n",
    "    def norm_value(self, current_value, start_value, drawdown_call, target_call, epsilon=1e-8):\n",
    "        \"\"\"Current value normalized in [-1,1] wrt upper and lower bounds\"\"\"\n",
    "        x = np.asarray(current_value)\n",
    "        x = (x / start_value - 1) * 100\n",
    "        x = (x - target_call)/(drawdown_call+target_call) + 1\n",
    "        x = 2 * np.clip(x, epsilon, 1 - epsilon) - 1 \n",
    "        return x\n",
    "    \n",
    "    def decayed_result(self, trade_result, current_value, start_value, drawdown_call, target_call, gamma=1.0):\n",
    "        \"\"\"Normalized in [-1,1] trade result, lineary decayed wrt current value.\"\"\"\n",
    "        target_value = start_value * (1 + target_call/100)\n",
    "        value_range = start_value * (drawdown_call + target_call)/100\n",
    "        decay = (gamma - 1) * (current_value - target_value) / value_range + gamma\n",
    "        x = trade_result * decay / value_range\n",
    "        return x\n",
    "    \n",
    "    def exp_scale(self, x, gamma=4):\n",
    "        \"\"\" Returns exp. scaled value in [0,1] for x in [0,1]; gamma controls steepness.\"\"\"\n",
    "        x = np.asarray(x) + 1\n",
    "        return(np.exp(x**gamma - 2**gamma))\n",
    "    \n",
    "    def get_unrealized_pnl(self, size=1):\n",
    "        \"\"\"\n",
    "        Normalized profit/loss for current opened trade (unrealized p/l).\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\"\n",
    "        x = np.asarray(self.stats.broker.value.get(size=size)) - self.realized_broker_value\n",
    "        self.unrealized_pnl = x / self.env.broker.startingcash /(self.p.drawdown_call + self.p.target_call) * 100\n",
    "        return self.unrealized_pnl\n",
    "    \n",
    "    def get_realized_pnl(self, size=1, gamma=1.0):\n",
    "        \"\"\"\n",
    "        Returns single trade realized profit/loss, normalized and gamma-adjusted to current broker value.\n",
    "        Returns vector of filled with p/l value of length `size`.\n",
    "        \"\"\"\n",
    "        if self.trade_just_closed:\n",
    "            self.realized_pnl = self.decayed_result(\n",
    "                self.trade_result,\n",
    "                np.asarray(self.stats.broker.value.get(size=size)),\n",
    "                self.env.broker.startingcash,\n",
    "                self.p.drawdown_call,\n",
    "                self.p.target_call,\n",
    "                gamma=gamma\n",
    "            ) \n",
    "            self.trade_just_closed = False\n",
    "        \n",
    "        else:\n",
    "            self.realized_pnl = np.zeros(size)\n",
    "            \n",
    "        #print('self.realized_pnl:', self.realized_pnl)  \n",
    "        return self.realized_pnl\n",
    "    \n",
    "    def get_norm_broker_value(self, size=1):\n",
    "        \"\"\"\n",
    "        Broker value, normalized in [-1,1] wrt profit/loss limits.\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\"\n",
    "        self.norm_broker_value = self.norm_value(\n",
    "            self.stats.broker.value.get(size=size),\n",
    "            self.env.broker.startingcash,\n",
    "            self.p.drawdown_call,\n",
    "            self.p.target_call,\n",
    "        )\n",
    "        return self.norm_broker_value \n",
    "    \n",
    "    def get_norm_broker_cash(self, size=1):\n",
    "        \"\"\"\n",
    "        Normalized broker cash.\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\" \n",
    "        self.norm_broker_cash = self.norm_value(\n",
    "            self.stats.broker.cash.get(size=size),\n",
    "            self.env.broker.startingcash,\n",
    "            99.0,\n",
    "            self.p.target_call,\n",
    "        )\n",
    "        return self.norm_broker_cash \n",
    "    \n",
    "    def get_norm_position_size(self, size=1):\n",
    "        \"\"\"\n",
    "        Normalized in (-1, 1) position size.\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\"\n",
    "        self.norm_position_size = np.asarray(\n",
    "            self.stats.position.exposure.get(size=size)\n",
    "        ) / (self.env.broker.startingcash * (np.asarray(self.stats.position.leverage.get(size=size)) + 1e-2))\n",
    "        return self.norm_position_size\n",
    "    \n",
    "    def get_norm_position_duration(self, size=1):\n",
    "        \"\"\"\n",
    "        Normalized in (0, 1) current position duration wrt max. episode steps.\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\"\n",
    "        self.norm_position_duration = np.frombuffer(self.stats.position.duration.get(size=size)) /\\\n",
    "            (self.data.numrecords - self.inner_embedding)\n",
    "        return self.norm_position_duration\n",
    "    \n",
    "    def get_norm_episode_duration(self, size=1):\n",
    "        \"\"\"\n",
    "        Normalized in (0, 1) current episode duration wrt max. episode steps.\n",
    "        Returns vector of treceback values of length `size`.\n",
    "        \"\"\"\n",
    "        self.norm_episode_duration = np.frombuffer(self.stats.reward.step.get(size=size)) /\\\n",
    "            (self.data.numrecords - self.inner_embedding)\n",
    "        return self.norm_episode_duration\n",
    "    \n",
    "    \n",
    "    def get_reward(self):\n",
    "        \"\"\"\n",
    "        Defines reward as weighted sum of portfolio performance statisitics\n",
    "        averaged over time-embedding period.\n",
    "        \"\"\"\n",
    "        # All reward terms for this step are already computed by get_state(), wich has been called just before.\n",
    "        \n",
    "        # Reward term 1: averaged profit/loss for current opened trade (unrealized p/l):\n",
    "        avg_unrealised_pnl = np.average(self.unrealized_pnl)\n",
    " \n",
    "        # Reward term 2: averaged broker value, normalized wrt to max drawdown and target bounds.\n",
    "        avg_norm_broker_value = np.average(self.norm_broker_value )\n",
    "        \n",
    "        # Reward term 3: normalized single trade realized profit/loss:\n",
    "        avg_realized_pnl = np.average(self.realized_pnl)\n",
    "        \n",
    "        avg_norm_episode_duration = np.average(self.norm_episode_duration)\n",
    "        abs_max_norm_position_size = abs(self.norm_position_size).max()\n",
    "        avg_norm_position_duration = np.average(self.norm_position_duration)\n",
    "            \n",
    "        # Weights are subject to tune, \n",
    "        # absolute reward walue should be consistent with a3c rollout length\n",
    "        # to keep gradients sane (~/2) while not slowing down training:\n",
    "        self.reward = (\n",
    "            + 1.0 * avg_unrealised_pnl\n",
    "            + 0.01 * avg_norm_broker_value \n",
    "            + 10.0 * avg_realized_pnl\n",
    "            - 5.0 * self.exp_scale(avg_norm_episode_duration, gamma=10) * abs_max_norm_position_size\n",
    "            #- 1.0 * self.exp_scale(avg_norm_position_duration, gamma=3)\n",
    "        ) / 2\n",
    "        \n",
    "        return self.reward \n",
    "\n",
    "class Position(bt.observer.Observer):\n",
    "    \"\"\" \n",
    "    Keeps track of position size and effective leverage.\n",
    "    \"\"\"\n",
    "    lines = ('exposure', 'leverage', 'duration')\n",
    "    plotinfo = dict(plot=True, subplot=True)\n",
    "    plotlines = dict(\n",
    "        exposure=dict(marker='.', markersize=1.0, color='blue', fillstyle='full'),\n",
    "        leverage=dict(_plotskip='True',),\n",
    "        duration=dict(_plotskip='True',),\n",
    "    )\n",
    "    current_duration = 0\n",
    "    \n",
    "    def next(self):\n",
    "        self.lines.exposure[0] = self._owner.position.size\n",
    "        self.lines.leverage[0] = self._owner.env.broker.get_leverage()\n",
    "        \n",
    "        if self._owner.position.size == 0:\n",
    "            self.current_duration = 0\n",
    "        \n",
    "        else:\n",
    "            self.current_duration += 1\n",
    "        self.lines.duration[0] = self.current_duration\n",
    "    \n",
    "    \n",
    "class Reward(bt.observer.Observer):\n",
    "    \"\"\" \n",
    "    Keeps track of reward values.\n",
    "    \"\"\"\n",
    "    lines = ('value', 'step')\n",
    "    plotinfo = dict(plot=True, subplot=True)\n",
    "    \n",
    "    plotlines = dict(\n",
    "        value=dict(markersize=4.0, color='green', fillstyle='full'),\n",
    "        step=dict(_plotskip='True',),\n",
    "    )\n",
    "    \n",
    "    def next(self):\n",
    "        self.lines.value[0] = self._owner.reward\n",
    "        self.lines.step[0] = self._owner.iteration\n",
    "\n",
    "########################################################       \n",
    "\n",
    "\n",
    "# Set backtesting engine parameters:\n",
    "\n",
    "time_embed_dim = 30\n",
    "\n",
    "state_shape = {\n",
    "    'raw_state': spaces.Box(low=-100, high=100, shape=(time_embed_dim, 4)),\n",
    "    'model_input': spaces.Box(low=-100, high=100, shape=(time_embed_dim, 9)),\n",
    "}\n",
    "\n",
    "MyCerebro = bt.Cerebro()\n",
    "\n",
    "MyCerebro.addstrategy(\n",
    "    MyStrategy,\n",
    "    state_shape=state_shape,\n",
    "    portfolio_actions=('hold', 'buy', 'sell', 'close'),\n",
    "    drawdown_call=5, # max % to loose, in percent of initial cash\n",
    "    target_call=15,  # max % to win, same\n",
    "    skip_frame=10,\n",
    ")\n",
    "\n",
    "# Set leveraged account:\n",
    "MyCerebro.broker.setcash(2000)\n",
    "MyCerebro.broker.setcommission(commission=0.0001, leverage=10.0) # commisssion to imitate spread\n",
    "MyCerebro.addsizer(bt.sizers.SizerFix, stake=5000,)  \n",
    "\n",
    "MyCerebro.addanalyzer(bt.analyzers.DrawDown)\n",
    "MyCerebro.addobserver(Reward)\n",
    "MyCerebro.addobserver(Position)\n",
    "\n",
    "MyDataset = BTgymDataset(\n",
    "    #filename='../data/DAT_ASCII_EURUSD_M1_2016.csv',\n",
    "    filename='../data/DAT_ASCII_EURUSD_M1_201703.csv',\n",
    "    #filename='../data/test_sine_1min_period256_delta0002.csv',\n",
    "    start_weekdays=[0, 1, 2, 3, 4],\n",
    "    episode_len_days=0,\n",
    "    episode_len_hours=23,\n",
    "    episode_len_minutes=55,\n",
    "    start_00=False,\n",
    "    time_gap_hours=6,\n",
    ")\n",
    "env_config = dict(\n",
    "    dataset=MyDataset,\n",
    "    engine=MyCerebro,\n",
    "    render_modes=['episode', 'human', 'model_input'],\n",
    "    render_state_as_image=True,\n",
    "    render_ylabel='OHL_diff channels and broker stat.',\n",
    "    render_size_episode=(12,8),\n",
    "    render_size_human=(10, 5),\n",
    "    render_size_state=(10, 5),\n",
    "    render_dpi=75,\n",
    "    port=5000,\n",
    "    data_port=4999,\n",
    "    connect_timeout=60,\n",
    "    verbose=0,\n",
    ")\n",
    "# Set tensorflow distributed cluster and a3c configuration:\n",
    "cluster_config = dict(\n",
    "    host='127.0.0.1',\n",
    "    port=12222,\n",
    "    num_workers=8,  # ~ num of CPU cores\n",
    "    num_ps=1,\n",
    "    log_dir='./tmp/a3c_test_4_2',\n",
    ")\n",
    "launcher = Launcher(\n",
    "    cluster_config=cluster_config,\n",
    "    env_class=BTgymEnv,\n",
    "    env_config=env_config,\n",
    "    policy_class=LSTMPolicy1D,\n",
    "    policy_config={'lstm_layers': (256,)}, # TODO: multi-layer, phased/grid LSTM\n",
    "    rollout_length=20,\n",
    "    model_beta=0.01,  # entropy regularization, TODO: make it log-unform over ~[0.1, 0.01]\n",
    "    opt_learn_rate=1e-4,  # adam learn rate,  TODO: RMSProp with shared stats, log-uniform learn rate + annealing\n",
    "    test_mode=False,\n",
    "    train_steps=1000000000,\n",
    "    model_summary_freq=20,\n",
    "    episode_summary_freq=1,\n",
    "    env_render_freq=10,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "launcher.run()\n",
    "\n",
    "# To track performance: [shell]:  tensorboard --logdir './tmp/a3c_test_4_2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
